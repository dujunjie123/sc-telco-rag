{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.9952,
  "eval_steps": 500,
  "global_step": 468,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.032,
      "grad_norm": 7.97076940536499,
      "learning_rate": 4.9985919525527434e-05,
      "loss": 5.4726,
      "num_input_tokens_seen": 57152,
      "step": 5
    },
    {
      "epoch": 0.064,
      "grad_norm": 7.375601768493652,
      "learning_rate": 4.994369396289063e-05,
      "loss": 3.3784,
      "num_input_tokens_seen": 114944,
      "step": 10
    },
    {
      "epoch": 0.096,
      "grad_norm": 2.9835903644561768,
      "learning_rate": 4.987337087656614e-05,
      "loss": 1.2174,
      "num_input_tokens_seen": 174016,
      "step": 15
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.8001253604888916,
      "learning_rate": 4.977502948114772e-05,
      "loss": 0.5288,
      "num_input_tokens_seen": 230528,
      "step": 20
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.7623900175094604,
      "learning_rate": 4.964878055211597e-05,
      "loss": 0.4074,
      "num_input_tokens_seen": 289728,
      "step": 25
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.9776433110237122,
      "learning_rate": 4.949476630105669e-05,
      "loss": 0.393,
      "num_input_tokens_seen": 348160,
      "step": 30
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.8126145601272583,
      "learning_rate": 4.9313160215468334e-05,
      "loss": 0.3811,
      "num_input_tokens_seen": 408512,
      "step": 35
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.7108922004699707,
      "learning_rate": 4.910416686333906e-05,
      "loss": 0.3793,
      "num_input_tokens_seen": 468096,
      "step": 40
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.9302847981452942,
      "learning_rate": 4.886802166271364e-05,
      "loss": 0.3849,
      "num_input_tokens_seen": 526208,
      "step": 45
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.8148182034492493,
      "learning_rate": 4.8604990616509616e-05,
      "loss": 0.357,
      "num_input_tokens_seen": 585344,
      "step": 50
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.9691699147224426,
      "learning_rate": 4.8315370012881514e-05,
      "loss": 0.3552,
      "num_input_tokens_seen": 646208,
      "step": 55
    },
    {
      "epoch": 0.384,
      "grad_norm": 1.2336894273757935,
      "learning_rate": 4.799948609147061e-05,
      "loss": 0.3551,
      "num_input_tokens_seen": 702720,
      "step": 60
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.8316941261291504,
      "learning_rate": 4.765769467591625e-05,
      "loss": 0.3433,
      "num_input_tokens_seen": 759040,
      "step": 65
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.6701808571815491,
      "learning_rate": 4.7290380773042575e-05,
      "loss": 0.3309,
      "num_input_tokens_seen": 817984,
      "step": 70
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.7401119470596313,
      "learning_rate": 4.68979581391722e-05,
      "loss": 0.3337,
      "num_input_tokens_seen": 874944,
      "step": 75
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.7878325581550598,
      "learning_rate": 4.6480868814055424e-05,
      "loss": 0.3301,
      "num_input_tokens_seen": 934336,
      "step": 80
    },
    {
      "epoch": 0.544,
      "grad_norm": 1.2454787492752075,
      "learning_rate": 4.6039582622939854e-05,
      "loss": 0.3182,
      "num_input_tokens_seen": 991360,
      "step": 85
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.8638529777526855,
      "learning_rate": 4.557459664734141e-05,
      "loss": 0.3364,
      "num_input_tokens_seen": 1050368,
      "step": 90
    },
    {
      "epoch": 0.608,
      "grad_norm": 0.8454008102416992,
      "learning_rate": 4.5086434665112864e-05,
      "loss": 0.2994,
      "num_input_tokens_seen": 1109184,
      "step": 95
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.8134446144104004,
      "learning_rate": 4.457564656044056e-05,
      "loss": 0.3139,
      "num_input_tokens_seen": 1166656,
      "step": 100
    },
    {
      "epoch": 0.672,
      "grad_norm": 0.7972493767738342,
      "learning_rate": 4.404280770443398e-05,
      "loss": 0.3423,
      "num_input_tokens_seen": 1224832,
      "step": 105
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.9869129657745361,
      "learning_rate": 4.348851830700593e-05,
      "loss": 0.3168,
      "num_input_tokens_seen": 1284160,
      "step": 110
    },
    {
      "epoch": 0.736,
      "grad_norm": 1.1230354309082031,
      "learning_rate": 4.2913402740773294e-05,
      "loss": 0.345,
      "num_input_tokens_seen": 1342016,
      "step": 115
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.8242865204811096,
      "learning_rate": 4.231810883773999e-05,
      "loss": 0.2848,
      "num_input_tokens_seen": 1402432,
      "step": 120
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.9524603486061096,
      "learning_rate": 4.170330715955444e-05,
      "loss": 0.3113,
      "num_input_tokens_seen": 1461184,
      "step": 125
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.8956403136253357,
      "learning_rate": 4.1069690242163484e-05,
      "loss": 0.3317,
      "num_input_tokens_seen": 1518144,
      "step": 130
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.8500530123710632,
      "learning_rate": 4.0417971815713584e-05,
      "loss": 0.3424,
      "num_input_tokens_seen": 1577344,
      "step": 135
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.9383653998374939,
      "learning_rate": 3.974888600057808e-05,
      "loss": 0.3299,
      "num_input_tokens_seen": 1635008,
      "step": 140
    },
    {
      "epoch": 0.928,
      "grad_norm": 1.1057206392288208,
      "learning_rate": 3.906318648041617e-05,
      "loss": 0.3348,
      "num_input_tokens_seen": 1694976,
      "step": 145
    },
    {
      "epoch": 0.96,
      "grad_norm": 1.0853108167648315,
      "learning_rate": 3.8361645653195026e-05,
      "loss": 0.3049,
      "num_input_tokens_seen": 1753216,
      "step": 150
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.8575101494789124,
      "learning_rate": 3.764505376113138e-05,
      "loss": 0.3363,
      "num_input_tokens_seen": 1811520,
      "step": 155
    },
    {
      "epoch": 1.024,
      "grad_norm": 0.8532353043556213,
      "learning_rate": 3.69142180005327e-05,
      "loss": 0.3093,
      "num_input_tokens_seen": 1870848,
      "step": 160
    },
    {
      "epoch": 1.056,
      "grad_norm": 0.9241156578063965,
      "learning_rate": 3.6169961612540645e-05,
      "loss": 0.3026,
      "num_input_tokens_seen": 1929024,
      "step": 165
    },
    {
      "epoch": 1.088,
      "grad_norm": 1.1604961156845093,
      "learning_rate": 3.5413122955801005e-05,
      "loss": 0.2982,
      "num_input_tokens_seen": 1986880,
      "step": 170
    },
    {
      "epoch": 1.12,
      "grad_norm": 1.0111312866210938,
      "learning_rate": 3.4644554562104634e-05,
      "loss": 0.3216,
      "num_input_tokens_seen": 2044032,
      "step": 175
    },
    {
      "epoch": 1.152,
      "grad_norm": 0.8877230286598206,
      "learning_rate": 3.386512217606339e-05,
      "loss": 0.3083,
      "num_input_tokens_seen": 2100480,
      "step": 180
    },
    {
      "epoch": 1.184,
      "grad_norm": 0.924007773399353,
      "learning_rate": 3.307570377990245e-05,
      "loss": 0.2965,
      "num_input_tokens_seen": 2161024,
      "step": 185
    },
    {
      "epoch": 1.216,
      "grad_norm": 0.8418046832084656,
      "learning_rate": 3.227718860446782e-05,
      "loss": 0.271,
      "num_input_tokens_seen": 2219520,
      "step": 190
    },
    {
      "epoch": 1.248,
      "grad_norm": 1.1502739191055298,
      "learning_rate": 3.147047612756302e-05,
      "loss": 0.2906,
      "num_input_tokens_seen": 2277632,
      "step": 195
    },
    {
      "epoch": 1.28,
      "grad_norm": 1.1246877908706665,
      "learning_rate": 3.065647506074306e-05,
      "loss": 0.2826,
      "num_input_tokens_seen": 2335872,
      "step": 200
    },
    {
      "epoch": 1.312,
      "grad_norm": 1.2048826217651367,
      "learning_rate": 2.983610232570728e-05,
      "loss": 0.2991,
      "num_input_tokens_seen": 2393408,
      "step": 205
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 1.0088621377944946,
      "learning_rate": 2.9010282021444008e-05,
      "loss": 0.3003,
      "num_input_tokens_seen": 2453312,
      "step": 210
    },
    {
      "epoch": 1.376,
      "grad_norm": 0.9417021870613098,
      "learning_rate": 2.8179944383290274e-05,
      "loss": 0.2841,
      "num_input_tokens_seen": 2515264,
      "step": 215
    },
    {
      "epoch": 1.408,
      "grad_norm": 0.9120087623596191,
      "learning_rate": 2.7346024735079486e-05,
      "loss": 0.3275,
      "num_input_tokens_seen": 2572480,
      "step": 220
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.8346737623214722,
      "learning_rate": 2.6509462435557152e-05,
      "loss": 0.3145,
      "num_input_tokens_seen": 2631168,
      "step": 225
    },
    {
      "epoch": 1.472,
      "grad_norm": 0.9186298847198486,
      "learning_rate": 2.5671199820251534e-05,
      "loss": 0.2966,
      "num_input_tokens_seen": 2689536,
      "step": 230
    },
    {
      "epoch": 1.504,
      "grad_norm": 1.0556186437606812,
      "learning_rate": 2.48321811399911e-05,
      "loss": 0.3077,
      "num_input_tokens_seen": 2744192,
      "step": 235
    },
    {
      "epoch": 1.536,
      "grad_norm": 1.2141692638397217,
      "learning_rate": 2.399335149726463e-05,
      "loss": 0.2896,
      "num_input_tokens_seen": 2802112,
      "step": 240
    },
    {
      "epoch": 1.568,
      "grad_norm": 1.1876133680343628,
      "learning_rate": 2.3155655781621793e-05,
      "loss": 0.3164,
      "num_input_tokens_seen": 2858688,
      "step": 245
    },
    {
      "epoch": 1.6,
      "grad_norm": 1.0950615406036377,
      "learning_rate": 2.2320037605313808e-05,
      "loss": 0.3072,
      "num_input_tokens_seen": 2916480,
      "step": 250
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 1.2472615242004395,
      "learning_rate": 2.148743824037269e-05,
      "loss": 0.2994,
      "num_input_tokens_seen": 2975232,
      "step": 255
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 1.0060091018676758,
      "learning_rate": 2.0658795558326743e-05,
      "loss": 0.3013,
      "num_input_tokens_seen": 3032448,
      "step": 260
    },
    {
      "epoch": 1.696,
      "grad_norm": 1.199604868888855,
      "learning_rate": 1.98350429737465e-05,
      "loss": 0.273,
      "num_input_tokens_seen": 3090752,
      "step": 265
    },
    {
      "epoch": 1.728,
      "grad_norm": 1.2445471286773682,
      "learning_rate": 1.9017108392811065e-05,
      "loss": 0.2665,
      "num_input_tokens_seen": 3150208,
      "step": 270
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.960944414138794,
      "learning_rate": 1.820591316807939e-05,
      "loss": 0.2852,
      "num_input_tokens_seen": 3210944,
      "step": 275
    },
    {
      "epoch": 1.792,
      "grad_norm": 1.2633131742477417,
      "learning_rate": 1.740237106064383e-05,
      "loss": 0.2996,
      "num_input_tokens_seen": 3267776,
      "step": 280
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 0.9962388873100281,
      "learning_rate": 1.6607387210834887e-05,
      "loss": 0.3096,
      "num_input_tokens_seen": 3324864,
      "step": 285
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 1.0167100429534912,
      "learning_rate": 1.582185711863681e-05,
      "loss": 0.2897,
      "num_input_tokens_seen": 3382272,
      "step": 290
    },
    {
      "epoch": 1.888,
      "grad_norm": 0.981644332408905,
      "learning_rate": 1.5046665634962476e-05,
      "loss": 0.2732,
      "num_input_tokens_seen": 3443200,
      "step": 295
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.9395761489868164,
      "learning_rate": 1.4282685964923642e-05,
      "loss": 0.299,
      "num_input_tokens_seen": 3502272,
      "step": 300
    },
    {
      "epoch": 1.952,
      "grad_norm": 1.0270731449127197,
      "learning_rate": 1.3530778684219648e-05,
      "loss": 0.3063,
      "num_input_tokens_seen": 3559040,
      "step": 305
    },
    {
      "epoch": 1.984,
      "grad_norm": 1.178871750831604,
      "learning_rate": 1.2791790769752232e-05,
      "loss": 0.3135,
      "num_input_tokens_seen": 3620352,
      "step": 310
    },
    {
      "epoch": 2.016,
      "grad_norm": 1.1713212728500366,
      "learning_rate": 1.2066554645558578e-05,
      "loss": 0.2979,
      "num_input_tokens_seen": 3678464,
      "step": 315
    },
    {
      "epoch": 2.048,
      "grad_norm": 1.0136812925338745,
      "learning_rate": 1.1355887245137383e-05,
      "loss": 0.3002,
      "num_input_tokens_seen": 3735360,
      "step": 320
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.9462032914161682,
      "learning_rate": 1.0660589091223855e-05,
      "loss": 0.2756,
      "num_input_tokens_seen": 3793344,
      "step": 325
    },
    {
      "epoch": 2.112,
      "grad_norm": 1.0833711624145508,
      "learning_rate": 9.981443394050525e-06,
      "loss": 0.265,
      "num_input_tokens_seen": 3849792,
      "step": 330
    },
    {
      "epoch": 2.144,
      "grad_norm": 1.2442752122879028,
      "learning_rate": 9.3192151691096e-06,
      "loss": 0.2851,
      "num_input_tokens_seen": 3908288,
      "step": 335
    },
    {
      "epoch": 2.176,
      "grad_norm": 1.4168564081192017,
      "learning_rate": 8.67465037541038e-06,
      "loss": 0.3197,
      "num_input_tokens_seen": 3967552,
      "step": 340
    },
    {
      "epoch": 2.208,
      "grad_norm": 0.9872523546218872,
      "learning_rate": 8.048475075202727e-06,
      "loss": 0.2639,
      "num_input_tokens_seen": 4027328,
      "step": 345
    },
    {
      "epoch": 2.24,
      "grad_norm": 1.0605695247650146,
      "learning_rate": 7.441394616113062e-06,
      "loss": 0.265,
      "num_input_tokens_seen": 4086528,
      "step": 350
    },
    {
      "epoch": 2.2720000000000002,
      "grad_norm": 1.0400303602218628,
      "learning_rate": 6.854092836613948e-06,
      "loss": 0.2485,
      "num_input_tokens_seen": 4143744,
      "step": 355
    },
    {
      "epoch": 2.304,
      "grad_norm": 1.0499738454818726,
      "learning_rate": 6.28723129572247e-06,
      "loss": 0.2687,
      "num_input_tokens_seen": 4202240,
      "step": 360
    },
    {
      "epoch": 2.336,
      "grad_norm": 0.9051024913787842,
      "learning_rate": 5.741448527795137e-06,
      "loss": 0.2924,
      "num_input_tokens_seen": 4260608,
      "step": 365
    },
    {
      "epoch": 2.368,
      "grad_norm": 1.396058201789856,
      "learning_rate": 5.217359323258459e-06,
      "loss": 0.3003,
      "num_input_tokens_seen": 4319808,
      "step": 370
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.9443534016609192,
      "learning_rate": 4.715554036085673e-06,
      "loss": 0.2837,
      "num_input_tokens_seen": 4376768,
      "step": 375
    },
    {
      "epoch": 2.432,
      "grad_norm": 1.0121891498565674,
      "learning_rate": 4.236597918799709e-06,
      "loss": 0.2741,
      "num_input_tokens_seen": 4437952,
      "step": 380
    },
    {
      "epoch": 2.464,
      "grad_norm": 1.315008521080017,
      "learning_rate": 3.7810304857511914e-06,
      "loss": 0.2885,
      "num_input_tokens_seen": 4494016,
      "step": 385
    },
    {
      "epoch": 2.496,
      "grad_norm": 1.0633453130722046,
      "learning_rate": 3.3493649053890326e-06,
      "loss": 0.2883,
      "num_input_tokens_seen": 4551744,
      "step": 390
    },
    {
      "epoch": 2.528,
      "grad_norm": 0.9376955628395081,
      "learning_rate": 2.942087422208051e-06,
      "loss": 0.2652,
      "num_input_tokens_seen": 4610560,
      "step": 395
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.9413621425628662,
      "learning_rate": 2.5596568090246548e-06,
      "loss": 0.2762,
      "num_input_tokens_seen": 4669568,
      "step": 400
    },
    {
      "epoch": 2.592,
      "grad_norm": 1.0300627946853638,
      "learning_rate": 2.2025038501977486e-06,
      "loss": 0.2524,
      "num_input_tokens_seen": 4728704,
      "step": 405
    },
    {
      "epoch": 2.624,
      "grad_norm": 0.9297987222671509,
      "learning_rate": 1.8710308563769124e-06,
      "loss": 0.2794,
      "num_input_tokens_seen": 4786624,
      "step": 410
    },
    {
      "epoch": 2.656,
      "grad_norm": 1.0753283500671387,
      "learning_rate": 1.5656112113243721e-06,
      "loss": 0.26,
      "num_input_tokens_seen": 4845440,
      "step": 415
    },
    {
      "epoch": 2.6879999999999997,
      "grad_norm": 1.1098741292953491,
      "learning_rate": 1.286588951321363e-06,
      "loss": 0.2608,
      "num_input_tokens_seen": 4903104,
      "step": 420
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 1.0952167510986328,
      "learning_rate": 1.034278377632636e-06,
      "loss": 0.279,
      "num_input_tokens_seen": 4963264,
      "step": 425
    },
    {
      "epoch": 2.752,
      "grad_norm": 1.3488997220993042,
      "learning_rate": 8.089637024655483e-07,
      "loss": 0.2818,
      "num_input_tokens_seen": 5021888,
      "step": 430
    },
    {
      "epoch": 2.784,
      "grad_norm": 1.1575883626937866,
      "learning_rate": 6.108987288226536e-07,
      "loss": 0.2914,
      "num_input_tokens_seen": 5078720,
      "step": 435
    },
    {
      "epoch": 2.816,
      "grad_norm": 1.064765453338623,
      "learning_rate": 4.403065646083809e-07,
      "loss": 0.2987,
      "num_input_tokens_seen": 5139648,
      "step": 440
    },
    {
      "epoch": 2.848,
      "grad_norm": 1.0904065370559692,
      "learning_rate": 2.973793713118039e-07,
      "loss": 0.2586,
      "num_input_tokens_seen": 5196608,
      "step": 445
    },
    {
      "epoch": 2.88,
      "grad_norm": 1.0761547088623047,
      "learning_rate": 1.8227814754865068e-07,
      "loss": 0.2725,
      "num_input_tokens_seen": 5253504,
      "step": 450
    },
    {
      "epoch": 2.912,
      "grad_norm": 1.0597492456436157,
      "learning_rate": 9.513254770636137e-08,
      "loss": 0.2958,
      "num_input_tokens_seen": 5312384,
      "step": 455
    },
    {
      "epoch": 2.944,
      "grad_norm": 1.0184128284454346,
      "learning_rate": 3.604073589645596e-08,
      "loss": 0.2822,
      "num_input_tokens_seen": 5370432,
      "step": 460
    },
    {
      "epoch": 2.976,
      "grad_norm": 1.4970234632492065,
      "learning_rate": 5.069275378746796e-09,
      "loss": 0.2858,
      "num_input_tokens_seen": 5430976,
      "step": 465
    },
    {
      "epoch": 2.9952,
      "num_input_tokens_seen": 5465152,
      "step": 468,
      "total_flos": 8.71942354010112e+16,
      "train_loss": 0.4043905999925401,
      "train_runtime": 1119.7684,
      "train_samples_per_second": 26.791,
      "train_steps_per_second": 0.418
    }
  ],
  "logging_steps": 5,
  "max_steps": 468,
  "num_input_tokens_seen": 5465152,
  "num_train_epochs": 3,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 8.71942354010112e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
