{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 1875,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.008,
      "grad_norm": 5.8502726554870605,
      "learning_rate": 4.999912270696202e-05,
      "loss": 8.7537,
      "num_input_tokens_seen": 38168,
      "step": 5
    },
    {
      "epoch": 0.016,
      "grad_norm": 8.590051651000977,
      "learning_rate": 4.9996490889419514e-05,
      "loss": 7.3507,
      "num_input_tokens_seen": 80576,
      "step": 10
    },
    {
      "epoch": 0.024,
      "grad_norm": 9.083951950073242,
      "learning_rate": 4.99921047320825e-05,
      "loss": 4.9005,
      "num_input_tokens_seen": 120912,
      "step": 15
    },
    {
      "epoch": 0.032,
      "grad_norm": 3.2630770206451416,
      "learning_rate": 4.9985964542786614e-05,
      "loss": 4.116,
      "num_input_tokens_seen": 160680,
      "step": 20
    },
    {
      "epoch": 0.04,
      "grad_norm": 2.413689613342285,
      "learning_rate": 4.997807075247146e-05,
      "loss": 3.7866,
      "num_input_tokens_seen": 202336,
      "step": 25
    },
    {
      "epoch": 0.048,
      "grad_norm": 2.971182346343994,
      "learning_rate": 4.996842391515044e-05,
      "loss": 3.483,
      "num_input_tokens_seen": 242904,
      "step": 30
    },
    {
      "epoch": 0.056,
      "grad_norm": 1.8091486692428589,
      "learning_rate": 4.9957024707871806e-05,
      "loss": 3.2589,
      "num_input_tokens_seen": 282392,
      "step": 35
    },
    {
      "epoch": 0.064,
      "grad_norm": 1.8243893384933472,
      "learning_rate": 4.994387393067117e-05,
      "loss": 3.0713,
      "num_input_tokens_seen": 324376,
      "step": 40
    },
    {
      "epoch": 0.072,
      "grad_norm": 1.4385868310928345,
      "learning_rate": 4.992897250651535e-05,
      "loss": 2.9093,
      "num_input_tokens_seen": 364392,
      "step": 45
    },
    {
      "epoch": 0.08,
      "grad_norm": 1.5806646347045898,
      "learning_rate": 4.991232148123761e-05,
      "loss": 2.8578,
      "num_input_tokens_seen": 404216,
      "step": 50
    },
    {
      "epoch": 0.088,
      "grad_norm": 1.4384762048721313,
      "learning_rate": 4.9893922023464236e-05,
      "loss": 2.7155,
      "num_input_tokens_seen": 444168,
      "step": 55
    },
    {
      "epoch": 0.096,
      "grad_norm": 1.8771940469741821,
      "learning_rate": 4.987377542453251e-05,
      "loss": 2.695,
      "num_input_tokens_seen": 485144,
      "step": 60
    },
    {
      "epoch": 0.104,
      "grad_norm": 1.9634432792663574,
      "learning_rate": 4.985188309840012e-05,
      "loss": 2.8019,
      "num_input_tokens_seen": 529536,
      "step": 65
    },
    {
      "epoch": 0.112,
      "grad_norm": 1.2356979846954346,
      "learning_rate": 4.982824658154589e-05,
      "loss": 2.7848,
      "num_input_tokens_seen": 572440,
      "step": 70
    },
    {
      "epoch": 0.12,
      "grad_norm": 1.445316195487976,
      "learning_rate": 4.980286753286195e-05,
      "loss": 2.6155,
      "num_input_tokens_seen": 612736,
      "step": 75
    },
    {
      "epoch": 0.128,
      "grad_norm": 1.8170732259750366,
      "learning_rate": 4.977574773353732e-05,
      "loss": 2.6181,
      "num_input_tokens_seen": 651600,
      "step": 80
    },
    {
      "epoch": 0.136,
      "grad_norm": 2.915144205093384,
      "learning_rate": 4.9746889086932895e-05,
      "loss": 2.6595,
      "num_input_tokens_seen": 692984,
      "step": 85
    },
    {
      "epoch": 0.144,
      "grad_norm": 1.9847122430801392,
      "learning_rate": 4.971629361844785e-05,
      "loss": 2.6374,
      "num_input_tokens_seen": 732472,
      "step": 90
    },
    {
      "epoch": 0.152,
      "grad_norm": 1.967124342918396,
      "learning_rate": 4.968396347537751e-05,
      "loss": 2.6486,
      "num_input_tokens_seen": 773144,
      "step": 95
    },
    {
      "epoch": 0.16,
      "grad_norm": 1.5355604887008667,
      "learning_rate": 4.964990092676263e-05,
      "loss": 2.5755,
      "num_input_tokens_seen": 813192,
      "step": 100
    },
    {
      "epoch": 0.168,
      "grad_norm": 1.6940442323684692,
      "learning_rate": 4.9614108363230135e-05,
      "loss": 2.6271,
      "num_input_tokens_seen": 853304,
      "step": 105
    },
    {
      "epoch": 0.176,
      "grad_norm": 2.0074574947357178,
      "learning_rate": 4.9576588296825386e-05,
      "loss": 2.5728,
      "num_input_tokens_seen": 893792,
      "step": 110
    },
    {
      "epoch": 0.184,
      "grad_norm": 2.0984277725219727,
      "learning_rate": 4.953734336083583e-05,
      "loss": 2.5961,
      "num_input_tokens_seen": 934480,
      "step": 115
    },
    {
      "epoch": 0.192,
      "grad_norm": 2.0204033851623535,
      "learning_rate": 4.949637630960617e-05,
      "loss": 2.5945,
      "num_input_tokens_seen": 974896,
      "step": 120
    },
    {
      "epoch": 0.2,
      "grad_norm": 1.8485839366912842,
      "learning_rate": 4.9453690018345144e-05,
      "loss": 2.6148,
      "num_input_tokens_seen": 1016376,
      "step": 125
    },
    {
      "epoch": 0.208,
      "grad_norm": 2.37060284614563,
      "learning_rate": 4.940928748292363e-05,
      "loss": 2.5989,
      "num_input_tokens_seen": 1057752,
      "step": 130
    },
    {
      "epoch": 0.216,
      "grad_norm": 1.7084704637527466,
      "learning_rate": 4.9363171819664434e-05,
      "loss": 2.6365,
      "num_input_tokens_seen": 1098848,
      "step": 135
    },
    {
      "epoch": 0.224,
      "grad_norm": 1.775359034538269,
      "learning_rate": 4.9315346265123594e-05,
      "loss": 2.6084,
      "num_input_tokens_seen": 1140320,
      "step": 140
    },
    {
      "epoch": 0.232,
      "grad_norm": 3.366687059402466,
      "learning_rate": 4.9265814175863186e-05,
      "loss": 2.7124,
      "num_input_tokens_seen": 1181360,
      "step": 145
    },
    {
      "epoch": 0.24,
      "grad_norm": 2.2338435649871826,
      "learning_rate": 4.9214579028215776e-05,
      "loss": 2.5869,
      "num_input_tokens_seen": 1222744,
      "step": 150
    },
    {
      "epoch": 0.248,
      "grad_norm": 1.7506595849990845,
      "learning_rate": 4.916164441804044e-05,
      "loss": 2.619,
      "num_input_tokens_seen": 1262488,
      "step": 155
    },
    {
      "epoch": 0.256,
      "grad_norm": 2.3887991905212402,
      "learning_rate": 4.910701406047037e-05,
      "loss": 2.6271,
      "num_input_tokens_seen": 1303784,
      "step": 160
    },
    {
      "epoch": 0.264,
      "grad_norm": 2.0496716499328613,
      "learning_rate": 4.905069178965215e-05,
      "loss": 2.6197,
      "num_input_tokens_seen": 1345920,
      "step": 165
    },
    {
      "epoch": 0.272,
      "grad_norm": 1.9288952350616455,
      "learning_rate": 4.899268155847667e-05,
      "loss": 2.5676,
      "num_input_tokens_seen": 1386280,
      "step": 170
    },
    {
      "epoch": 0.28,
      "grad_norm": 1.8818217515945435,
      "learning_rate": 4.893298743830168e-05,
      "loss": 2.6224,
      "num_input_tokens_seen": 1427160,
      "step": 175
    },
    {
      "epoch": 0.288,
      "grad_norm": 1.8098416328430176,
      "learning_rate": 4.887161361866608e-05,
      "loss": 2.6524,
      "num_input_tokens_seen": 1468888,
      "step": 180
    },
    {
      "epoch": 0.296,
      "grad_norm": 1.6402982473373413,
      "learning_rate": 4.880856440699582e-05,
      "loss": 2.58,
      "num_input_tokens_seen": 1508648,
      "step": 185
    },
    {
      "epoch": 0.304,
      "grad_norm": 1.4134411811828613,
      "learning_rate": 4.874384422830167e-05,
      "loss": 2.5919,
      "num_input_tokens_seen": 1548304,
      "step": 190
    },
    {
      "epoch": 0.312,
      "grad_norm": 1.3592443466186523,
      "learning_rate": 4.867745762486861e-05,
      "loss": 2.6176,
      "num_input_tokens_seen": 1588208,
      "step": 195
    },
    {
      "epoch": 0.32,
      "grad_norm": 2.851900339126587,
      "learning_rate": 4.860940925593703e-05,
      "loss": 2.6684,
      "num_input_tokens_seen": 1628608,
      "step": 200
    },
    {
      "epoch": 0.328,
      "grad_norm": 1.5819592475891113,
      "learning_rate": 4.8539703897375755e-05,
      "loss": 2.5211,
      "num_input_tokens_seen": 1668544,
      "step": 205
    },
    {
      "epoch": 0.336,
      "grad_norm": 1.98728346824646,
      "learning_rate": 4.846834644134686e-05,
      "loss": 2.7009,
      "num_input_tokens_seen": 1707304,
      "step": 210
    },
    {
      "epoch": 0.344,
      "grad_norm": 4.286101818084717,
      "learning_rate": 4.839534189596228e-05,
      "loss": 2.7593,
      "num_input_tokens_seen": 1750384,
      "step": 215
    },
    {
      "epoch": 0.352,
      "grad_norm": 1.6668847799301147,
      "learning_rate": 4.832069538493237e-05,
      "loss": 2.6088,
      "num_input_tokens_seen": 1790800,
      "step": 220
    },
    {
      "epoch": 0.36,
      "grad_norm": 1.6001335382461548,
      "learning_rate": 4.8244412147206284e-05,
      "loss": 2.5614,
      "num_input_tokens_seen": 1831648,
      "step": 225
    },
    {
      "epoch": 0.368,
      "grad_norm": 1.3206136226654053,
      "learning_rate": 4.81664975366043e-05,
      "loss": 2.5412,
      "num_input_tokens_seen": 1871736,
      "step": 230
    },
    {
      "epoch": 0.376,
      "grad_norm": 1.0841469764709473,
      "learning_rate": 4.808695702144206e-05,
      "loss": 2.6135,
      "num_input_tokens_seen": 1913368,
      "step": 235
    },
    {
      "epoch": 0.384,
      "grad_norm": 1.6546306610107422,
      "learning_rate": 4.800579618414676e-05,
      "loss": 2.575,
      "num_input_tokens_seen": 1954992,
      "step": 240
    },
    {
      "epoch": 0.392,
      "grad_norm": 1.6640015840530396,
      "learning_rate": 4.7923020720865414e-05,
      "loss": 2.5676,
      "num_input_tokens_seen": 1996272,
      "step": 245
    },
    {
      "epoch": 0.4,
      "grad_norm": 2.0822904109954834,
      "learning_rate": 4.783863644106502e-05,
      "loss": 2.5017,
      "num_input_tokens_seen": 2036896,
      "step": 250
    },
    {
      "epoch": 0.408,
      "grad_norm": 1.8335140943527222,
      "learning_rate": 4.775264926712489e-05,
      "loss": 2.6107,
      "num_input_tokens_seen": 2075792,
      "step": 255
    },
    {
      "epoch": 0.416,
      "grad_norm": 1.4782907962799072,
      "learning_rate": 4.7665065233920945e-05,
      "loss": 2.6808,
      "num_input_tokens_seen": 2118648,
      "step": 260
    },
    {
      "epoch": 0.424,
      "grad_norm": 1.356353521347046,
      "learning_rate": 4.7575890488402185e-05,
      "loss": 2.6058,
      "num_input_tokens_seen": 2158848,
      "step": 265
    },
    {
      "epoch": 0.432,
      "grad_norm": 1.1626644134521484,
      "learning_rate": 4.7485131289159276e-05,
      "loss": 2.6087,
      "num_input_tokens_seen": 2199400,
      "step": 270
    },
    {
      "epoch": 0.44,
      "grad_norm": 1.3854681253433228,
      "learning_rate": 4.7392794005985326e-05,
      "loss": 2.5357,
      "num_input_tokens_seen": 2241280,
      "step": 275
    },
    {
      "epoch": 0.448,
      "grad_norm": 2.0467288494110107,
      "learning_rate": 4.7298885119428773e-05,
      "loss": 2.6055,
      "num_input_tokens_seen": 2283152,
      "step": 280
    },
    {
      "epoch": 0.456,
      "grad_norm": 1.4693608283996582,
      "learning_rate": 4.720341122033862e-05,
      "loss": 2.5561,
      "num_input_tokens_seen": 2326808,
      "step": 285
    },
    {
      "epoch": 0.464,
      "grad_norm": 1.1451565027236938,
      "learning_rate": 4.710637900940181e-05,
      "loss": 2.6145,
      "num_input_tokens_seen": 2369416,
      "step": 290
    },
    {
      "epoch": 0.472,
      "grad_norm": 1.6219441890716553,
      "learning_rate": 4.7007795296673006e-05,
      "loss": 2.5828,
      "num_input_tokens_seen": 2411128,
      "step": 295
    },
    {
      "epoch": 0.48,
      "grad_norm": 3.783034324645996,
      "learning_rate": 4.690766700109659e-05,
      "loss": 2.5715,
      "num_input_tokens_seen": 2452168,
      "step": 300
    },
    {
      "epoch": 0.488,
      "grad_norm": 1.439923644065857,
      "learning_rate": 4.68060011500211e-05,
      "loss": 2.5881,
      "num_input_tokens_seen": 2492712,
      "step": 305
    },
    {
      "epoch": 0.496,
      "grad_norm": 2.047616958618164,
      "learning_rate": 4.670280487870598e-05,
      "loss": 2.6167,
      "num_input_tokens_seen": 2532200,
      "step": 310
    },
    {
      "epoch": 0.504,
      "grad_norm": 1.3504140377044678,
      "learning_rate": 4.659808542982088e-05,
      "loss": 2.5304,
      "num_input_tokens_seen": 2572440,
      "step": 315
    },
    {
      "epoch": 0.512,
      "grad_norm": 1.0852547883987427,
      "learning_rate": 4.649185015293728e-05,
      "loss": 2.6166,
      "num_input_tokens_seen": 2612864,
      "step": 320
    },
    {
      "epoch": 0.52,
      "grad_norm": 1.2994589805603027,
      "learning_rate": 4.638410650401267e-05,
      "loss": 2.6044,
      "num_input_tokens_seen": 2654352,
      "step": 325
    },
    {
      "epoch": 0.528,
      "grad_norm": 1.7274616956710815,
      "learning_rate": 4.6274862044867304e-05,
      "loss": 2.5421,
      "num_input_tokens_seen": 2695480,
      "step": 330
    },
    {
      "epoch": 0.536,
      "grad_norm": 1.7937512397766113,
      "learning_rate": 4.616412444265345e-05,
      "loss": 2.5801,
      "num_input_tokens_seen": 2736904,
      "step": 335
    },
    {
      "epoch": 0.544,
      "grad_norm": 1.4986881017684937,
      "learning_rate": 4.605190146931731e-05,
      "loss": 2.6449,
      "num_input_tokens_seen": 2779464,
      "step": 340
    },
    {
      "epoch": 0.552,
      "grad_norm": 1.3634198904037476,
      "learning_rate": 4.593820100105355e-05,
      "loss": 2.5581,
      "num_input_tokens_seen": 2818952,
      "step": 345
    },
    {
      "epoch": 0.56,
      "grad_norm": 1.2399227619171143,
      "learning_rate": 4.5823031017752485e-05,
      "loss": 2.577,
      "num_input_tokens_seen": 2860344,
      "step": 350
    },
    {
      "epoch": 0.568,
      "grad_norm": 1.2519298791885376,
      "learning_rate": 4.5706399602440106e-05,
      "loss": 2.5267,
      "num_input_tokens_seen": 2902344,
      "step": 355
    },
    {
      "epoch": 0.576,
      "grad_norm": 1.5881274938583374,
      "learning_rate": 4.558831494071069e-05,
      "loss": 2.6604,
      "num_input_tokens_seen": 2943416,
      "step": 360
    },
    {
      "epoch": 0.584,
      "grad_norm": 2.4097177982330322,
      "learning_rate": 4.5468785320152365e-05,
      "loss": 2.654,
      "num_input_tokens_seen": 2983872,
      "step": 365
    },
    {
      "epoch": 0.592,
      "grad_norm": 1.0673481225967407,
      "learning_rate": 4.534781912976546e-05,
      "loss": 2.482,
      "num_input_tokens_seen": 3025912,
      "step": 370
    },
    {
      "epoch": 0.6,
      "grad_norm": 2.147735834121704,
      "learning_rate": 4.522542485937369e-05,
      "loss": 2.5357,
      "num_input_tokens_seen": 3066832,
      "step": 375
    },
    {
      "epoch": 0.608,
      "grad_norm": 1.5976675748825073,
      "learning_rate": 4.510161109902837e-05,
      "loss": 2.4838,
      "num_input_tokens_seen": 3107560,
      "step": 380
    },
    {
      "epoch": 0.616,
      "grad_norm": 1.8334153890609741,
      "learning_rate": 4.4976386538405495e-05,
      "loss": 2.5926,
      "num_input_tokens_seen": 3148440,
      "step": 385
    },
    {
      "epoch": 0.624,
      "grad_norm": 3.3585283756256104,
      "learning_rate": 4.484975996619589e-05,
      "loss": 2.5753,
      "num_input_tokens_seen": 3190424,
      "step": 390
    },
    {
      "epoch": 0.632,
      "grad_norm": 2.0034213066101074,
      "learning_rate": 4.4721740269488355e-05,
      "loss": 2.5566,
      "num_input_tokens_seen": 3230888,
      "step": 395
    },
    {
      "epoch": 0.64,
      "grad_norm": 1.1792865991592407,
      "learning_rate": 4.4592336433146e-05,
      "loss": 2.5724,
      "num_input_tokens_seen": 3272008,
      "step": 400
    },
    {
      "epoch": 0.648,
      "grad_norm": 1.6678260564804077,
      "learning_rate": 4.4461557539175594e-05,
      "loss": 2.579,
      "num_input_tokens_seen": 3312344,
      "step": 405
    },
    {
      "epoch": 0.656,
      "grad_norm": 2.5859813690185547,
      "learning_rate": 4.432941276609018e-05,
      "loss": 2.6224,
      "num_input_tokens_seen": 3352728,
      "step": 410
    },
    {
      "epoch": 0.664,
      "grad_norm": 1.1126757860183716,
      "learning_rate": 4.4195911388264946e-05,
      "loss": 2.5811,
      "num_input_tokens_seen": 3393064,
      "step": 415
    },
    {
      "epoch": 0.672,
      "grad_norm": 1.4209327697753906,
      "learning_rate": 4.40610627752862e-05,
      "loss": 2.5794,
      "num_input_tokens_seen": 3435192,
      "step": 420
    },
    {
      "epoch": 0.68,
      "grad_norm": 1.2389920949935913,
      "learning_rate": 4.3924876391293915e-05,
      "loss": 2.5615,
      "num_input_tokens_seen": 3474328,
      "step": 425
    },
    {
      "epoch": 0.688,
      "grad_norm": 1.5911964178085327,
      "learning_rate": 4.3787361794317405e-05,
      "loss": 2.5746,
      "num_input_tokens_seen": 3515392,
      "step": 430
    },
    {
      "epoch": 0.696,
      "grad_norm": 1.565598487854004,
      "learning_rate": 4.3648528635604556e-05,
      "loss": 2.6177,
      "num_input_tokens_seen": 3555968,
      "step": 435
    },
    {
      "epoch": 0.704,
      "grad_norm": 1.0859956741333008,
      "learning_rate": 4.350838665894446e-05,
      "loss": 2.5813,
      "num_input_tokens_seen": 3597336,
      "step": 440
    },
    {
      "epoch": 0.712,
      "grad_norm": 2.037400007247925,
      "learning_rate": 4.336694569998354e-05,
      "loss": 2.5778,
      "num_input_tokens_seen": 3637152,
      "step": 445
    },
    {
      "epoch": 0.72,
      "grad_norm": 1.2696226835250854,
      "learning_rate": 4.3224215685535294e-05,
      "loss": 2.5123,
      "num_input_tokens_seen": 3677608,
      "step": 450
    },
    {
      "epoch": 0.728,
      "grad_norm": 1.6721473932266235,
      "learning_rate": 4.3080206632883554e-05,
      "loss": 2.5749,
      "num_input_tokens_seen": 3717696,
      "step": 455
    },
    {
      "epoch": 0.736,
      "grad_norm": 2.271939754486084,
      "learning_rate": 4.293492864907947e-05,
      "loss": 2.6492,
      "num_input_tokens_seen": 3759976,
      "step": 460
    },
    {
      "epoch": 0.744,
      "grad_norm": 1.3462202548980713,
      "learning_rate": 4.278839193023214e-05,
      "loss": 2.5431,
      "num_input_tokens_seen": 3802472,
      "step": 465
    },
    {
      "epoch": 0.752,
      "grad_norm": 1.6890549659729004,
      "learning_rate": 4.264060676079302e-05,
      "loss": 2.595,
      "num_input_tokens_seen": 3843248,
      "step": 470
    },
    {
      "epoch": 0.76,
      "grad_norm": 1.0253580808639526,
      "learning_rate": 4.249158351283414e-05,
      "loss": 2.5075,
      "num_input_tokens_seen": 3883128,
      "step": 475
    },
    {
      "epoch": 0.768,
      "grad_norm": 1.836841106414795,
      "learning_rate": 4.234133264532012e-05,
      "loss": 2.5129,
      "num_input_tokens_seen": 3924344,
      "step": 480
    },
    {
      "epoch": 0.776,
      "grad_norm": 1.9004771709442139,
      "learning_rate": 4.218986470337419e-05,
      "loss": 2.5322,
      "num_input_tokens_seen": 3965904,
      "step": 485
    },
    {
      "epoch": 0.784,
      "grad_norm": 2.030046224594116,
      "learning_rate": 4.2037190317538e-05,
      "loss": 2.533,
      "num_input_tokens_seen": 4006080,
      "step": 490
    },
    {
      "epoch": 0.792,
      "grad_norm": 1.3876687288284302,
      "learning_rate": 4.188332020302561e-05,
      "loss": 2.5937,
      "num_input_tokens_seen": 4045800,
      "step": 495
    },
    {
      "epoch": 0.8,
      "grad_norm": 1.1754016876220703,
      "learning_rate": 4.172826515897146e-05,
      "loss": 2.5103,
      "num_input_tokens_seen": 4088624,
      "step": 500
    },
    {
      "epoch": 0.808,
      "grad_norm": 1.3158961534500122,
      "learning_rate": 4.157203606767238e-05,
      "loss": 2.6151,
      "num_input_tokens_seen": 4128632,
      "step": 505
    },
    {
      "epoch": 0.816,
      "grad_norm": 5.424674034118652,
      "learning_rate": 4.1414643893823914e-05,
      "loss": 2.7834,
      "num_input_tokens_seen": 4170000,
      "step": 510
    },
    {
      "epoch": 0.824,
      "grad_norm": 1.4389690160751343,
      "learning_rate": 4.125609968375072e-05,
      "loss": 2.5184,
      "num_input_tokens_seen": 4209656,
      "step": 515
    },
    {
      "epoch": 0.832,
      "grad_norm": 1.2635658979415894,
      "learning_rate": 4.109641456463135e-05,
      "loss": 2.5085,
      "num_input_tokens_seen": 4249376,
      "step": 520
    },
    {
      "epoch": 0.84,
      "grad_norm": 1.546921730041504,
      "learning_rate": 4.093559974371725e-05,
      "loss": 2.6236,
      "num_input_tokens_seen": 4290688,
      "step": 525
    },
    {
      "epoch": 0.848,
      "grad_norm": 1.404805302619934,
      "learning_rate": 4.077366650754624e-05,
      "loss": 2.5954,
      "num_input_tokens_seen": 4331728,
      "step": 530
    },
    {
      "epoch": 0.856,
      "grad_norm": 1.665040373802185,
      "learning_rate": 4.0610626221150394e-05,
      "loss": 2.5346,
      "num_input_tokens_seen": 4371320,
      "step": 535
    },
    {
      "epoch": 0.864,
      "grad_norm": 1.6980032920837402,
      "learning_rate": 4.044649032725836e-05,
      "loss": 2.5659,
      "num_input_tokens_seen": 4410888,
      "step": 540
    },
    {
      "epoch": 0.872,
      "grad_norm": 1.243074893951416,
      "learning_rate": 4.028127034549229e-05,
      "loss": 2.5555,
      "num_input_tokens_seen": 4451512,
      "step": 545
    },
    {
      "epoch": 0.88,
      "grad_norm": 1.037954568862915,
      "learning_rate": 4.011497787155938e-05,
      "loss": 2.5299,
      "num_input_tokens_seen": 4491536,
      "step": 550
    },
    {
      "epoch": 0.888,
      "grad_norm": 1.6840325593948364,
      "learning_rate": 3.9947624576437975e-05,
      "loss": 2.5449,
      "num_input_tokens_seen": 4531608,
      "step": 555
    },
    {
      "epoch": 0.896,
      "grad_norm": 1.5354852676391602,
      "learning_rate": 3.977922220555855e-05,
      "loss": 2.595,
      "num_input_tokens_seen": 4570880,
      "step": 560
    },
    {
      "epoch": 0.904,
      "grad_norm": 1.8532586097717285,
      "learning_rate": 3.960978257797931e-05,
      "loss": 2.5777,
      "num_input_tokens_seen": 4610928,
      "step": 565
    },
    {
      "epoch": 0.912,
      "grad_norm": 1.6736822128295898,
      "learning_rate": 3.943931758555669e-05,
      "loss": 2.547,
      "num_input_tokens_seen": 4651768,
      "step": 570
    },
    {
      "epoch": 0.92,
      "grad_norm": 1.279057502746582,
      "learning_rate": 3.92678391921108e-05,
      "loss": 2.6246,
      "num_input_tokens_seen": 4693264,
      "step": 575
    },
    {
      "epoch": 0.928,
      "grad_norm": 1.5512527227401733,
      "learning_rate": 3.909535943258567e-05,
      "loss": 2.5859,
      "num_input_tokens_seen": 4734200,
      "step": 580
    },
    {
      "epoch": 0.936,
      "grad_norm": 1.4692797660827637,
      "learning_rate": 3.8921890412204705e-05,
      "loss": 2.5352,
      "num_input_tokens_seen": 4776416,
      "step": 585
    },
    {
      "epoch": 0.944,
      "grad_norm": 1.4922423362731934,
      "learning_rate": 3.8747444305621e-05,
      "loss": 2.5312,
      "num_input_tokens_seen": 4817864,
      "step": 590
    },
    {
      "epoch": 0.952,
      "grad_norm": 1.2379124164581299,
      "learning_rate": 3.8572033356062943e-05,
      "loss": 2.5717,
      "num_input_tokens_seen": 4858600,
      "step": 595
    },
    {
      "epoch": 0.96,
      "grad_norm": 1.5584468841552734,
      "learning_rate": 3.8395669874474915e-05,
      "loss": 2.5603,
      "num_input_tokens_seen": 4900016,
      "step": 600
    },
    {
      "epoch": 0.968,
      "grad_norm": 1.7835026979446411,
      "learning_rate": 3.821836623865329e-05,
      "loss": 2.658,
      "num_input_tokens_seen": 4939648,
      "step": 605
    },
    {
      "epoch": 0.976,
      "grad_norm": 1.9113640785217285,
      "learning_rate": 3.80401348923777e-05,
      "loss": 2.6641,
      "num_input_tokens_seen": 4981368,
      "step": 610
    },
    {
      "epoch": 0.984,
      "grad_norm": 1.7503796815872192,
      "learning_rate": 3.786098834453766e-05,
      "loss": 2.5101,
      "num_input_tokens_seen": 5022112,
      "step": 615
    },
    {
      "epoch": 0.992,
      "grad_norm": 1.1355451345443726,
      "learning_rate": 3.7680939168254733e-05,
      "loss": 2.5788,
      "num_input_tokens_seen": 5062536,
      "step": 620
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.3293713331222534,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 2.5518,
      "num_input_tokens_seen": 5104424,
      "step": 625
    },
    {
      "epoch": 1.008,
      "grad_norm": 1.1987580060958862,
      "learning_rate": 3.731818353870729e-05,
      "loss": 2.5474,
      "num_input_tokens_seen": 5145728,
      "step": 630
    },
    {
      "epoch": 1.016,
      "grad_norm": 1.3424503803253174,
      "learning_rate": 3.713550254488185e-05,
      "loss": 2.5302,
      "num_input_tokens_seen": 5187056,
      "step": 635
    },
    {
      "epoch": 1.024,
      "grad_norm": 1.4917455911636353,
      "learning_rate": 3.695196983970481e-05,
      "loss": 2.5397,
      "num_input_tokens_seen": 5227192,
      "step": 640
    },
    {
      "epoch": 1.032,
      "grad_norm": 1.8537850379943848,
      "learning_rate": 3.6767598304133324e-05,
      "loss": 2.5697,
      "num_input_tokens_seen": 5266008,
      "step": 645
    },
    {
      "epoch": 1.04,
      "grad_norm": 1.9778110980987549,
      "learning_rate": 3.6582400877996546e-05,
      "loss": 2.5818,
      "num_input_tokens_seen": 5308720,
      "step": 650
    },
    {
      "epoch": 1.048,
      "grad_norm": 1.4160265922546387,
      "learning_rate": 3.639639055908751e-05,
      "loss": 2.4983,
      "num_input_tokens_seen": 5349248,
      "step": 655
    },
    {
      "epoch": 1.056,
      "grad_norm": 1.3251543045043945,
      "learning_rate": 3.6209580402250815e-05,
      "loss": 2.5372,
      "num_input_tokens_seen": 5388912,
      "step": 660
    },
    {
      "epoch": 1.064,
      "grad_norm": 1.6181113719940186,
      "learning_rate": 3.602198351846647e-05,
      "loss": 2.542,
      "num_input_tokens_seen": 5430696,
      "step": 665
    },
    {
      "epoch": 1.072,
      "grad_norm": 2.1579675674438477,
      "learning_rate": 3.5833613073929684e-05,
      "loss": 2.542,
      "num_input_tokens_seen": 5470672,
      "step": 670
    },
    {
      "epoch": 1.08,
      "grad_norm": 1.5639758110046387,
      "learning_rate": 3.564448228912682e-05,
      "loss": 2.5583,
      "num_input_tokens_seen": 5510736,
      "step": 675
    },
    {
      "epoch": 1.088,
      "grad_norm": 1.5023647546768188,
      "learning_rate": 3.545460443790753e-05,
      "loss": 2.5387,
      "num_input_tokens_seen": 5551400,
      "step": 680
    },
    {
      "epoch": 1.096,
      "grad_norm": 2.283275842666626,
      "learning_rate": 3.52639928465532e-05,
      "loss": 2.6079,
      "num_input_tokens_seen": 5594224,
      "step": 685
    },
    {
      "epoch": 1.104,
      "grad_norm": 1.438878059387207,
      "learning_rate": 3.507266089284157e-05,
      "loss": 2.496,
      "num_input_tokens_seen": 5634000,
      "step": 690
    },
    {
      "epoch": 1.112,
      "grad_norm": 1.4804821014404297,
      "learning_rate": 3.488062200510791e-05,
      "loss": 2.5552,
      "num_input_tokens_seen": 5674736,
      "step": 695
    },
    {
      "epoch": 1.12,
      "grad_norm": 1.8592385053634644,
      "learning_rate": 3.4687889661302576e-05,
      "loss": 2.5534,
      "num_input_tokens_seen": 5715432,
      "step": 700
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 1.802919864654541,
      "learning_rate": 3.4494477388045035e-05,
      "loss": 2.5784,
      "num_input_tokens_seen": 5755552,
      "step": 705
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 1.656350016593933,
      "learning_rate": 3.430039875967454e-05,
      "loss": 2.5633,
      "num_input_tokens_seen": 5795704,
      "step": 710
    },
    {
      "epoch": 1.144,
      "grad_norm": 1.871001124382019,
      "learning_rate": 3.410566739729746e-05,
      "loss": 2.548,
      "num_input_tokens_seen": 5836688,
      "step": 715
    },
    {
      "epoch": 1.152,
      "grad_norm": 1.9383172988891602,
      "learning_rate": 3.3910296967831266e-05,
      "loss": 2.5715,
      "num_input_tokens_seen": 5876904,
      "step": 720
    },
    {
      "epoch": 1.16,
      "grad_norm": 1.8271254301071167,
      "learning_rate": 3.3714301183045385e-05,
      "loss": 2.5389,
      "num_input_tokens_seen": 5918552,
      "step": 725
    },
    {
      "epoch": 1.168,
      "grad_norm": 1.2904881238937378,
      "learning_rate": 3.35176937985988e-05,
      "loss": 2.4814,
      "num_input_tokens_seen": 5958616,
      "step": 730
    },
    {
      "epoch": 1.176,
      "grad_norm": 1.9593924283981323,
      "learning_rate": 3.332048861307467e-05,
      "loss": 2.5676,
      "num_input_tokens_seen": 5998264,
      "step": 735
    },
    {
      "epoch": 1.184,
      "grad_norm": 1.601889967918396,
      "learning_rate": 3.312269946701191e-05,
      "loss": 2.5659,
      "num_input_tokens_seen": 6039376,
      "step": 740
    },
    {
      "epoch": 1.192,
      "grad_norm": 0.9759429693222046,
      "learning_rate": 3.29243402419338e-05,
      "loss": 2.4991,
      "num_input_tokens_seen": 6080688,
      "step": 745
    },
    {
      "epoch": 1.2,
      "grad_norm": 1.8479207754135132,
      "learning_rate": 3.272542485937369e-05,
      "loss": 2.5448,
      "num_input_tokens_seen": 6119920,
      "step": 750
    },
    {
      "epoch": 1.208,
      "grad_norm": 1.8110370635986328,
      "learning_rate": 3.2525967279898015e-05,
      "loss": 2.5324,
      "num_input_tokens_seen": 6161048,
      "step": 755
    },
    {
      "epoch": 1.216,
      "grad_norm": 2.480750799179077,
      "learning_rate": 3.2325981502126433e-05,
      "loss": 2.5324,
      "num_input_tokens_seen": 6202696,
      "step": 760
    },
    {
      "epoch": 1.224,
      "grad_norm": 1.2686572074890137,
      "learning_rate": 3.21254815617494e-05,
      "loss": 2.5383,
      "num_input_tokens_seen": 6244032,
      "step": 765
    },
    {
      "epoch": 1.232,
      "grad_norm": 2.0060875415802,
      "learning_rate": 3.192448153054306e-05,
      "loss": 2.5309,
      "num_input_tokens_seen": 6285200,
      "step": 770
    },
    {
      "epoch": 1.24,
      "grad_norm": 1.1508198976516724,
      "learning_rate": 3.172299551538164e-05,
      "loss": 2.5207,
      "num_input_tokens_seen": 6325336,
      "step": 775
    },
    {
      "epoch": 1.248,
      "grad_norm": 1.961037278175354,
      "learning_rate": 3.152103765724743e-05,
      "loss": 2.5769,
      "num_input_tokens_seen": 6367784,
      "step": 780
    },
    {
      "epoch": 1.256,
      "grad_norm": 1.5143479108810425,
      "learning_rate": 3.1318622130238236e-05,
      "loss": 2.5892,
      "num_input_tokens_seen": 6409840,
      "step": 785
    },
    {
      "epoch": 1.264,
      "grad_norm": 2.0288097858428955,
      "learning_rate": 3.111576314057268e-05,
      "loss": 2.5518,
      "num_input_tokens_seen": 6450608,
      "step": 790
    },
    {
      "epoch": 1.272,
      "grad_norm": 1.4507865905761719,
      "learning_rate": 3.091247492559312e-05,
      "loss": 2.5061,
      "num_input_tokens_seen": 6491648,
      "step": 795
    },
    {
      "epoch": 1.28,
      "grad_norm": 2.442798137664795,
      "learning_rate": 3.0708771752766394e-05,
      "loss": 2.4915,
      "num_input_tokens_seen": 6531240,
      "step": 800
    },
    {
      "epoch": 1.288,
      "grad_norm": 2.272531747817993,
      "learning_rate": 3.050466791868254e-05,
      "loss": 2.4979,
      "num_input_tokens_seen": 6572560,
      "step": 805
    },
    {
      "epoch": 1.296,
      "grad_norm": 1.3137058019638062,
      "learning_rate": 3.0300177748051373e-05,
      "loss": 2.6067,
      "num_input_tokens_seen": 6614104,
      "step": 810
    },
    {
      "epoch": 1.304,
      "grad_norm": 1.8066335916519165,
      "learning_rate": 3.0095315592697126e-05,
      "loss": 2.5518,
      "num_input_tokens_seen": 6654576,
      "step": 815
    },
    {
      "epoch": 1.312,
      "grad_norm": 1.8425525426864624,
      "learning_rate": 2.9890095830551207e-05,
      "loss": 2.5467,
      "num_input_tokens_seen": 6695936,
      "step": 820
    },
    {
      "epoch": 1.32,
      "grad_norm": 1.7477271556854248,
      "learning_rate": 2.9684532864643122e-05,
      "loss": 2.5693,
      "num_input_tokens_seen": 6737424,
      "step": 825
    },
    {
      "epoch": 1.328,
      "grad_norm": 1.5327818393707275,
      "learning_rate": 2.9478641122089562e-05,
      "loss": 2.5289,
      "num_input_tokens_seen": 6777744,
      "step": 830
    },
    {
      "epoch": 1.336,
      "grad_norm": 1.4975115060806274,
      "learning_rate": 2.9272435053081922e-05,
      "loss": 2.509,
      "num_input_tokens_seen": 6819712,
      "step": 835
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 1.5647155046463013,
      "learning_rate": 2.9065929129872094e-05,
      "loss": 2.5392,
      "num_input_tokens_seen": 6859560,
      "step": 840
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 1.9043077230453491,
      "learning_rate": 2.8859137845756784e-05,
      "loss": 2.5073,
      "num_input_tokens_seen": 6900744,
      "step": 845
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 1.7678951025009155,
      "learning_rate": 2.8652075714060295e-05,
      "loss": 2.5041,
      "num_input_tokens_seen": 6942200,
      "step": 850
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 1.2845991849899292,
      "learning_rate": 2.844475726711595e-05,
      "loss": 2.5854,
      "num_input_tokens_seen": 6982984,
      "step": 855
    },
    {
      "epoch": 1.376,
      "grad_norm": 2.379812479019165,
      "learning_rate": 2.8237197055246172e-05,
      "loss": 2.5403,
      "num_input_tokens_seen": 7021864,
      "step": 860
    },
    {
      "epoch": 1.384,
      "grad_norm": 1.5360227823257446,
      "learning_rate": 2.8029409645741267e-05,
      "loss": 2.5624,
      "num_input_tokens_seen": 7062064,
      "step": 865
    },
    {
      "epoch": 1.392,
      "grad_norm": 1.3786031007766724,
      "learning_rate": 2.782140962183704e-05,
      "loss": 2.571,
      "num_input_tokens_seen": 7102872,
      "step": 870
    },
    {
      "epoch": 1.4,
      "grad_norm": 1.3310747146606445,
      "learning_rate": 2.761321158169134e-05,
      "loss": 2.5803,
      "num_input_tokens_seen": 7143016,
      "step": 875
    },
    {
      "epoch": 1.408,
      "grad_norm": 1.8229806423187256,
      "learning_rate": 2.7404830137359444e-05,
      "loss": 2.5853,
      "num_input_tokens_seen": 7183480,
      "step": 880
    },
    {
      "epoch": 1.416,
      "grad_norm": 1.2802597284317017,
      "learning_rate": 2.7196279913768584e-05,
      "loss": 2.4909,
      "num_input_tokens_seen": 7223880,
      "step": 885
    },
    {
      "epoch": 1.424,
      "grad_norm": 2.6745989322662354,
      "learning_rate": 2.6987575547691497e-05,
      "loss": 2.6173,
      "num_input_tokens_seen": 7264320,
      "step": 890
    },
    {
      "epoch": 1.432,
      "grad_norm": 1.4026803970336914,
      "learning_rate": 2.6778731686719178e-05,
      "loss": 2.5334,
      "num_input_tokens_seen": 7304880,
      "step": 895
    },
    {
      "epoch": 1.44,
      "grad_norm": 1.2084838151931763,
      "learning_rate": 2.656976298823284e-05,
      "loss": 2.5947,
      "num_input_tokens_seen": 7344176,
      "step": 900
    },
    {
      "epoch": 1.448,
      "grad_norm": 1.5645742416381836,
      "learning_rate": 2.636068411837523e-05,
      "loss": 2.5512,
      "num_input_tokens_seen": 7383760,
      "step": 905
    },
    {
      "epoch": 1.456,
      "grad_norm": 0.9765833616256714,
      "learning_rate": 2.615150975102131e-05,
      "loss": 2.5418,
      "num_input_tokens_seen": 7423992,
      "step": 910
    },
    {
      "epoch": 1.464,
      "grad_norm": 1.729727864265442,
      "learning_rate": 2.594225456674837e-05,
      "loss": 2.5331,
      "num_input_tokens_seen": 7465528,
      "step": 915
    },
    {
      "epoch": 1.472,
      "grad_norm": 2.0124549865722656,
      "learning_rate": 2.5732933251805713e-05,
      "loss": 2.5085,
      "num_input_tokens_seen": 7506592,
      "step": 920
    },
    {
      "epoch": 1.48,
      "grad_norm": 1.7755708694458008,
      "learning_rate": 2.5523560497083926e-05,
      "loss": 2.5721,
      "num_input_tokens_seen": 7547392,
      "step": 925
    },
    {
      "epoch": 1.488,
      "grad_norm": 1.5194405317306519,
      "learning_rate": 2.531415099708382e-05,
      "loss": 2.5616,
      "num_input_tokens_seen": 7586896,
      "step": 930
    },
    {
      "epoch": 1.496,
      "grad_norm": 1.532185435295105,
      "learning_rate": 2.51047194488851e-05,
      "loss": 2.5613,
      "num_input_tokens_seen": 7627168,
      "step": 935
    },
    {
      "epoch": 1.504,
      "grad_norm": 1.395865559577942,
      "learning_rate": 2.4895280551114907e-05,
      "loss": 2.5683,
      "num_input_tokens_seen": 7668448,
      "step": 940
    },
    {
      "epoch": 1.512,
      "grad_norm": 1.4570186138153076,
      "learning_rate": 2.4685849002916183e-05,
      "loss": 2.5095,
      "num_input_tokens_seen": 7708952,
      "step": 945
    },
    {
      "epoch": 1.52,
      "grad_norm": 1.8621330261230469,
      "learning_rate": 2.447643950291608e-05,
      "loss": 2.5357,
      "num_input_tokens_seen": 7749568,
      "step": 950
    },
    {
      "epoch": 1.528,
      "grad_norm": 1.224411129951477,
      "learning_rate": 2.4267066748194296e-05,
      "loss": 2.5396,
      "num_input_tokens_seen": 7788864,
      "step": 955
    },
    {
      "epoch": 1.536,
      "grad_norm": 2.066441535949707,
      "learning_rate": 2.4057745433251635e-05,
      "loss": 2.5192,
      "num_input_tokens_seen": 7829176,
      "step": 960
    },
    {
      "epoch": 1.544,
      "grad_norm": 1.3380496501922607,
      "learning_rate": 2.384849024897869e-05,
      "loss": 2.5762,
      "num_input_tokens_seen": 7869424,
      "step": 965
    },
    {
      "epoch": 1.552,
      "grad_norm": 1.4800928831100464,
      "learning_rate": 2.3639315881624777e-05,
      "loss": 2.5818,
      "num_input_tokens_seen": 7910688,
      "step": 970
    },
    {
      "epoch": 1.56,
      "grad_norm": 2.452026128768921,
      "learning_rate": 2.3430237011767167e-05,
      "loss": 2.6205,
      "num_input_tokens_seen": 7952864,
      "step": 975
    },
    {
      "epoch": 1.568,
      "grad_norm": 1.8414973020553589,
      "learning_rate": 2.3221268313280838e-05,
      "loss": 2.5324,
      "num_input_tokens_seen": 7993032,
      "step": 980
    },
    {
      "epoch": 1.576,
      "grad_norm": 1.759176254272461,
      "learning_rate": 2.301242445230851e-05,
      "loss": 2.5481,
      "num_input_tokens_seen": 8033952,
      "step": 985
    },
    {
      "epoch": 1.584,
      "grad_norm": 2.2242705821990967,
      "learning_rate": 2.280372008623142e-05,
      "loss": 2.6014,
      "num_input_tokens_seen": 8076096,
      "step": 990
    },
    {
      "epoch": 1.592,
      "grad_norm": 1.380255937576294,
      "learning_rate": 2.2595169862640568e-05,
      "loss": 2.4636,
      "num_input_tokens_seen": 8118112,
      "step": 995
    },
    {
      "epoch": 1.6,
      "grad_norm": 2.2397584915161133,
      "learning_rate": 2.238678841830867e-05,
      "loss": 2.5257,
      "num_input_tokens_seen": 8158584,
      "step": 1000
    },
    {
      "epoch": 1.608,
      "grad_norm": 1.759717583656311,
      "learning_rate": 2.217859037816296e-05,
      "loss": 2.5092,
      "num_input_tokens_seen": 8199616,
      "step": 1005
    },
    {
      "epoch": 1.616,
      "grad_norm": 1.7555161714553833,
      "learning_rate": 2.1970590354258745e-05,
      "loss": 2.5088,
      "num_input_tokens_seen": 8241064,
      "step": 1010
    },
    {
      "epoch": 1.624,
      "grad_norm": 1.534274697303772,
      "learning_rate": 2.176280294475383e-05,
      "loss": 2.5618,
      "num_input_tokens_seen": 8281840,
      "step": 1015
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 2.063404083251953,
      "learning_rate": 2.155524273288405e-05,
      "loss": 2.5495,
      "num_input_tokens_seen": 8322336,
      "step": 1020
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 1.2547367811203003,
      "learning_rate": 2.1347924285939714e-05,
      "loss": 2.4956,
      "num_input_tokens_seen": 8361496,
      "step": 1025
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 1.5256267786026,
      "learning_rate": 2.114086215424322e-05,
      "loss": 2.5731,
      "num_input_tokens_seen": 8402240,
      "step": 1030
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 1.8783206939697266,
      "learning_rate": 2.0934070870127912e-05,
      "loss": 2.5779,
      "num_input_tokens_seen": 8441792,
      "step": 1035
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 1.8454456329345703,
      "learning_rate": 2.0727564946918087e-05,
      "loss": 2.5409,
      "num_input_tokens_seen": 8481224,
      "step": 1040
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 1.5596200227737427,
      "learning_rate": 2.0521358877910444e-05,
      "loss": 2.5213,
      "num_input_tokens_seen": 8521392,
      "step": 1045
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 1.270915150642395,
      "learning_rate": 2.031546713535688e-05,
      "loss": 2.4956,
      "num_input_tokens_seen": 8561200,
      "step": 1050
    },
    {
      "epoch": 1.688,
      "grad_norm": 1.0442254543304443,
      "learning_rate": 2.01099041694488e-05,
      "loss": 2.4734,
      "num_input_tokens_seen": 8601112,
      "step": 1055
    },
    {
      "epoch": 1.696,
      "grad_norm": 1.1872344017028809,
      "learning_rate": 1.9904684407302883e-05,
      "loss": 2.5375,
      "num_input_tokens_seen": 8642128,
      "step": 1060
    },
    {
      "epoch": 1.704,
      "grad_norm": 1.2592623233795166,
      "learning_rate": 1.969982225194864e-05,
      "loss": 2.494,
      "num_input_tokens_seen": 8684888,
      "step": 1065
    },
    {
      "epoch": 1.712,
      "grad_norm": 0.9016655683517456,
      "learning_rate": 1.9495332081317464e-05,
      "loss": 2.4308,
      "num_input_tokens_seen": 8725352,
      "step": 1070
    },
    {
      "epoch": 1.72,
      "grad_norm": 2.624361276626587,
      "learning_rate": 1.9291228247233605e-05,
      "loss": 2.5065,
      "num_input_tokens_seen": 8767736,
      "step": 1075
    },
    {
      "epoch": 1.728,
      "grad_norm": 2.7781002521514893,
      "learning_rate": 1.908752507440689e-05,
      "loss": 2.5348,
      "num_input_tokens_seen": 8809472,
      "step": 1080
    },
    {
      "epoch": 1.736,
      "grad_norm": 2.055950880050659,
      "learning_rate": 1.888423685942732e-05,
      "loss": 2.6381,
      "num_input_tokens_seen": 8849904,
      "step": 1085
    },
    {
      "epoch": 1.744,
      "grad_norm": 2.3233044147491455,
      "learning_rate": 1.868137786976177e-05,
      "loss": 2.5242,
      "num_input_tokens_seen": 8893080,
      "step": 1090
    },
    {
      "epoch": 1.752,
      "grad_norm": 1.6123697757720947,
      "learning_rate": 1.8478962342752583e-05,
      "loss": 2.5151,
      "num_input_tokens_seen": 8932736,
      "step": 1095
    },
    {
      "epoch": 1.76,
      "grad_norm": 1.368826985359192,
      "learning_rate": 1.827700448461836e-05,
      "loss": 2.5109,
      "num_input_tokens_seen": 8973240,
      "step": 1100
    },
    {
      "epoch": 1.768,
      "grad_norm": 1.456704020500183,
      "learning_rate": 1.807551846945694e-05,
      "loss": 2.5009,
      "num_input_tokens_seen": 9012584,
      "step": 1105
    },
    {
      "epoch": 1.776,
      "grad_norm": 2.144150495529175,
      "learning_rate": 1.7874518438250597e-05,
      "loss": 2.5742,
      "num_input_tokens_seen": 9054408,
      "step": 1110
    },
    {
      "epoch": 1.784,
      "grad_norm": 1.6895495653152466,
      "learning_rate": 1.767401849787357e-05,
      "loss": 2.5534,
      "num_input_tokens_seen": 9094880,
      "step": 1115
    },
    {
      "epoch": 1.792,
      "grad_norm": 1.0468692779541016,
      "learning_rate": 1.747403272010199e-05,
      "loss": 2.5181,
      "num_input_tokens_seen": 9135160,
      "step": 1120
    },
    {
      "epoch": 1.8,
      "grad_norm": 1.4166724681854248,
      "learning_rate": 1.7274575140626318e-05,
      "loss": 2.514,
      "num_input_tokens_seen": 9174568,
      "step": 1125
    },
    {
      "epoch": 1.808,
      "grad_norm": 2.1243951320648193,
      "learning_rate": 1.7075659758066208e-05,
      "loss": 2.5593,
      "num_input_tokens_seen": 9214240,
      "step": 1130
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 1.6049896478652954,
      "learning_rate": 1.6877300532988094e-05,
      "loss": 2.5381,
      "num_input_tokens_seen": 9254640,
      "step": 1135
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 2.3394691944122314,
      "learning_rate": 1.6679511386925337e-05,
      "loss": 2.517,
      "num_input_tokens_seen": 9296904,
      "step": 1140
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 1.5020866394042969,
      "learning_rate": 1.648230620140121e-05,
      "loss": 2.5379,
      "num_input_tokens_seen": 9338432,
      "step": 1145
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 1.2922269105911255,
      "learning_rate": 1.6285698816954624e-05,
      "loss": 2.606,
      "num_input_tokens_seen": 9378744,
      "step": 1150
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 0.9069033265113831,
      "learning_rate": 1.6089703032168733e-05,
      "loss": 2.4661,
      "num_input_tokens_seen": 9419960,
      "step": 1155
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 1.5185308456420898,
      "learning_rate": 1.5894332602702545e-05,
      "loss": 2.515,
      "num_input_tokens_seen": 9460984,
      "step": 1160
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 2.2306149005889893,
      "learning_rate": 1.5699601240325474e-05,
      "loss": 2.5075,
      "num_input_tokens_seen": 9501736,
      "step": 1165
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 1.550418496131897,
      "learning_rate": 1.5505522611954975e-05,
      "loss": 2.4772,
      "num_input_tokens_seen": 9542992,
      "step": 1170
    },
    {
      "epoch": 1.88,
      "grad_norm": 2.3458659648895264,
      "learning_rate": 1.5312110338697426e-05,
      "loss": 2.5289,
      "num_input_tokens_seen": 9584128,
      "step": 1175
    },
    {
      "epoch": 1.888,
      "grad_norm": 1.8755569458007812,
      "learning_rate": 1.5119377994892094e-05,
      "loss": 2.5192,
      "num_input_tokens_seen": 9624824,
      "step": 1180
    },
    {
      "epoch": 1.896,
      "grad_norm": 1.3140535354614258,
      "learning_rate": 1.4927339107158437e-05,
      "loss": 2.5492,
      "num_input_tokens_seen": 9665920,
      "step": 1185
    },
    {
      "epoch": 1.904,
      "grad_norm": 1.29922616481781,
      "learning_rate": 1.4736007153446801e-05,
      "loss": 2.5167,
      "num_input_tokens_seen": 9708280,
      "step": 1190
    },
    {
      "epoch": 1.912,
      "grad_norm": 0.76278156042099,
      "learning_rate": 1.4545395562092468e-05,
      "loss": 2.5599,
      "num_input_tokens_seen": 9749688,
      "step": 1195
    },
    {
      "epoch": 1.92,
      "grad_norm": 1.5432085990905762,
      "learning_rate": 1.4355517710873184e-05,
      "loss": 2.5474,
      "num_input_tokens_seen": 9792808,
      "step": 1200
    },
    {
      "epoch": 1.928,
      "grad_norm": 2.1143810749053955,
      "learning_rate": 1.4166386926070322e-05,
      "loss": 2.5521,
      "num_input_tokens_seen": 9833912,
      "step": 1205
    },
    {
      "epoch": 1.936,
      "grad_norm": 1.8219048976898193,
      "learning_rate": 1.397801648153354e-05,
      "loss": 2.5147,
      "num_input_tokens_seen": 9873744,
      "step": 1210
    },
    {
      "epoch": 1.944,
      "grad_norm": 2.0547358989715576,
      "learning_rate": 1.3790419597749199e-05,
      "loss": 2.5848,
      "num_input_tokens_seen": 9916920,
      "step": 1215
    },
    {
      "epoch": 1.952,
      "grad_norm": 1.615208625793457,
      "learning_rate": 1.3603609440912507e-05,
      "loss": 2.572,
      "num_input_tokens_seen": 9957944,
      "step": 1220
    },
    {
      "epoch": 1.96,
      "grad_norm": 2.2482752799987793,
      "learning_rate": 1.3417599122003464e-05,
      "loss": 2.5351,
      "num_input_tokens_seen": 9998888,
      "step": 1225
    },
    {
      "epoch": 1.968,
      "grad_norm": 2.154392719268799,
      "learning_rate": 1.3232401695866687e-05,
      "loss": 2.5277,
      "num_input_tokens_seen": 10041464,
      "step": 1230
    },
    {
      "epoch": 1.976,
      "grad_norm": 1.3906968832015991,
      "learning_rate": 1.3048030160295196e-05,
      "loss": 2.5201,
      "num_input_tokens_seen": 10082872,
      "step": 1235
    },
    {
      "epoch": 1.984,
      "grad_norm": 2.1569011211395264,
      "learning_rate": 1.2864497455118152e-05,
      "loss": 2.5278,
      "num_input_tokens_seen": 10125992,
      "step": 1240
    },
    {
      "epoch": 1.992,
      "grad_norm": 2.1612186431884766,
      "learning_rate": 1.2681816461292715e-05,
      "loss": 2.5741,
      "num_input_tokens_seen": 10166616,
      "step": 1245
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.2579606771469116,
      "learning_rate": 1.2500000000000006e-05,
      "loss": 2.5174,
      "num_input_tokens_seen": 10208848,
      "step": 1250
    },
    {
      "epoch": 2.008,
      "grad_norm": 1.705247402191162,
      "learning_rate": 1.2319060831745272e-05,
      "loss": 2.5006,
      "num_input_tokens_seen": 10249584,
      "step": 1255
    },
    {
      "epoch": 2.016,
      "grad_norm": 1.537834644317627,
      "learning_rate": 1.2139011655462337e-05,
      "loss": 2.5212,
      "num_input_tokens_seen": 10291344,
      "step": 1260
    },
    {
      "epoch": 2.024,
      "grad_norm": 1.8129189014434814,
      "learning_rate": 1.1959865107622307e-05,
      "loss": 2.487,
      "num_input_tokens_seen": 10331080,
      "step": 1265
    },
    {
      "epoch": 2.032,
      "grad_norm": 2.0856075286865234,
      "learning_rate": 1.1781633761346707e-05,
      "loss": 2.5324,
      "num_input_tokens_seen": 10371760,
      "step": 1270
    },
    {
      "epoch": 2.04,
      "grad_norm": 2.229644775390625,
      "learning_rate": 1.1604330125525079e-05,
      "loss": 2.5434,
      "num_input_tokens_seen": 10410888,
      "step": 1275
    },
    {
      "epoch": 2.048,
      "grad_norm": 1.3614428043365479,
      "learning_rate": 1.1427966643937069e-05,
      "loss": 2.5109,
      "num_input_tokens_seen": 10450376,
      "step": 1280
    },
    {
      "epoch": 2.056,
      "grad_norm": 1.5175132751464844,
      "learning_rate": 1.1252555694379006e-05,
      "loss": 2.5264,
      "num_input_tokens_seen": 10492048,
      "step": 1285
    },
    {
      "epoch": 2.064,
      "grad_norm": 2.4873998165130615,
      "learning_rate": 1.107810958779531e-05,
      "loss": 2.5196,
      "num_input_tokens_seen": 10531520,
      "step": 1290
    },
    {
      "epoch": 2.072,
      "grad_norm": 1.6265058517456055,
      "learning_rate": 1.0904640567414332e-05,
      "loss": 2.5006,
      "num_input_tokens_seen": 10572240,
      "step": 1295
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.9788028597831726,
      "learning_rate": 1.0732160807889211e-05,
      "loss": 2.4556,
      "num_input_tokens_seen": 10612992,
      "step": 1300
    },
    {
      "epoch": 2.088,
      "grad_norm": 1.473256230354309,
      "learning_rate": 1.0560682414443315e-05,
      "loss": 2.4977,
      "num_input_tokens_seen": 10653400,
      "step": 1305
    },
    {
      "epoch": 2.096,
      "grad_norm": 1.12481689453125,
      "learning_rate": 1.03902174220207e-05,
      "loss": 2.4548,
      "num_input_tokens_seen": 10693576,
      "step": 1310
    },
    {
      "epoch": 2.104,
      "grad_norm": 1.296799898147583,
      "learning_rate": 1.022077779444145e-05,
      "loss": 2.4654,
      "num_input_tokens_seen": 10734040,
      "step": 1315
    },
    {
      "epoch": 2.112,
      "grad_norm": 2.6860318183898926,
      "learning_rate": 1.0052375423562038e-05,
      "loss": 2.5468,
      "num_input_tokens_seen": 10776456,
      "step": 1320
    },
    {
      "epoch": 2.12,
      "grad_norm": 2.2774770259857178,
      "learning_rate": 9.88502212844063e-06,
      "loss": 2.5358,
      "num_input_tokens_seen": 10819944,
      "step": 1325
    },
    {
      "epoch": 2.128,
      "grad_norm": 1.4287588596343994,
      "learning_rate": 9.718729654507713e-06,
      "loss": 2.5052,
      "num_input_tokens_seen": 10861848,
      "step": 1330
    },
    {
      "epoch": 2.136,
      "grad_norm": 2.299272298812866,
      "learning_rate": 9.553509672741645e-06,
      "loss": 2.506,
      "num_input_tokens_seen": 10901648,
      "step": 1335
    },
    {
      "epoch": 2.144,
      "grad_norm": 2.2448084354400635,
      "learning_rate": 9.389373778849612e-06,
      "loss": 2.5748,
      "num_input_tokens_seen": 10941672,
      "step": 1340
    },
    {
      "epoch": 2.152,
      "grad_norm": 1.7863860130310059,
      "learning_rate": 9.22633349245376e-06,
      "loss": 2.4585,
      "num_input_tokens_seen": 10983696,
      "step": 1345
    },
    {
      "epoch": 2.16,
      "grad_norm": 2.518885612487793,
      "learning_rate": 9.064400256282757e-06,
      "loss": 2.5195,
      "num_input_tokens_seen": 11023952,
      "step": 1350
    },
    {
      "epoch": 2.168,
      "grad_norm": 1.6554532051086426,
      "learning_rate": 8.903585435368658e-06,
      "loss": 2.5368,
      "num_input_tokens_seen": 11064336,
      "step": 1355
    },
    {
      "epoch": 2.176,
      "grad_norm": 2.8208694458007812,
      "learning_rate": 8.743900316249273e-06,
      "loss": 2.5945,
      "num_input_tokens_seen": 11106032,
      "step": 1360
    },
    {
      "epoch": 2.184,
      "grad_norm": 2.9146828651428223,
      "learning_rate": 8.585356106176094e-06,
      "loss": 2.4853,
      "num_input_tokens_seen": 11150080,
      "step": 1365
    },
    {
      "epoch": 2.192,
      "grad_norm": 1.6474599838256836,
      "learning_rate": 8.42796393232762e-06,
      "loss": 2.4802,
      "num_input_tokens_seen": 11190872,
      "step": 1370
    },
    {
      "epoch": 2.2,
      "grad_norm": 1.615810513496399,
      "learning_rate": 8.271734841028553e-06,
      "loss": 2.5147,
      "num_input_tokens_seen": 11230592,
      "step": 1375
    },
    {
      "epoch": 2.208,
      "grad_norm": 1.8709603548049927,
      "learning_rate": 8.116679796974388e-06,
      "loss": 2.4807,
      "num_input_tokens_seen": 11271392,
      "step": 1380
    },
    {
      "epoch": 2.216,
      "grad_norm": 2.40384840965271,
      "learning_rate": 7.962809682462009e-06,
      "loss": 2.5118,
      "num_input_tokens_seen": 11313256,
      "step": 1385
    },
    {
      "epoch": 2.224,
      "grad_norm": 2.6902949810028076,
      "learning_rate": 7.810135296625818e-06,
      "loss": 2.4976,
      "num_input_tokens_seen": 11351592,
      "step": 1390
    },
    {
      "epoch": 2.232,
      "grad_norm": 2.16062593460083,
      "learning_rate": 7.65866735467988e-06,
      "loss": 2.4559,
      "num_input_tokens_seen": 11392696,
      "step": 1395
    },
    {
      "epoch": 2.24,
      "grad_norm": 2.5979549884796143,
      "learning_rate": 7.508416487165862e-06,
      "loss": 2.5405,
      "num_input_tokens_seen": 11433112,
      "step": 1400
    },
    {
      "epoch": 2.248,
      "grad_norm": 1.5243701934814453,
      "learning_rate": 7.359393239206991e-06,
      "loss": 2.469,
      "num_input_tokens_seen": 11474544,
      "step": 1405
    },
    {
      "epoch": 2.2560000000000002,
      "grad_norm": 2.0915956497192383,
      "learning_rate": 7.211608069767867e-06,
      "loss": 2.4539,
      "num_input_tokens_seen": 11515992,
      "step": 1410
    },
    {
      "epoch": 2.2640000000000002,
      "grad_norm": 1.432672381401062,
      "learning_rate": 7.065071350920538e-06,
      "loss": 2.56,
      "num_input_tokens_seen": 11556208,
      "step": 1415
    },
    {
      "epoch": 2.2720000000000002,
      "grad_norm": 1.6525195837020874,
      "learning_rate": 6.919793367116453e-06,
      "loss": 2.4888,
      "num_input_tokens_seen": 11596992,
      "step": 1420
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 1.6009324789047241,
      "learning_rate": 6.775784314464717e-06,
      "loss": 2.4871,
      "num_input_tokens_seen": 11636856,
      "step": 1425
    },
    {
      "epoch": 2.288,
      "grad_norm": 1.4713696241378784,
      "learning_rate": 6.6330543000164645e-06,
      "loss": 2.5062,
      "num_input_tokens_seen": 11677648,
      "step": 1430
    },
    {
      "epoch": 2.296,
      "grad_norm": 2.508056640625,
      "learning_rate": 6.4916133410555466e-06,
      "loss": 2.5192,
      "num_input_tokens_seen": 11717488,
      "step": 1435
    },
    {
      "epoch": 2.304,
      "grad_norm": 2.177151918411255,
      "learning_rate": 6.3514713643954475e-06,
      "loss": 2.492,
      "num_input_tokens_seen": 11757624,
      "step": 1440
    },
    {
      "epoch": 2.312,
      "grad_norm": 1.8090753555297852,
      "learning_rate": 6.2126382056826e-06,
      "loss": 2.5439,
      "num_input_tokens_seen": 11799392,
      "step": 1445
    },
    {
      "epoch": 2.32,
      "grad_norm": 1.52989661693573,
      "learning_rate": 6.075123608706093e-06,
      "loss": 2.4937,
      "num_input_tokens_seen": 11839080,
      "step": 1450
    },
    {
      "epoch": 2.328,
      "grad_norm": 1.509507417678833,
      "learning_rate": 5.9389372247138e-06,
      "loss": 2.4769,
      "num_input_tokens_seen": 11879664,
      "step": 1455
    },
    {
      "epoch": 2.336,
      "grad_norm": 1.7798514366149902,
      "learning_rate": 5.80408861173507e-06,
      "loss": 2.4649,
      "num_input_tokens_seen": 11919240,
      "step": 1460
    },
    {
      "epoch": 2.344,
      "grad_norm": 2.4587182998657227,
      "learning_rate": 5.6705872339098186e-06,
      "loss": 2.5496,
      "num_input_tokens_seen": 11960144,
      "step": 1465
    },
    {
      "epoch": 2.352,
      "grad_norm": 1.5441237688064575,
      "learning_rate": 5.538442460824417e-06,
      "loss": 2.4991,
      "num_input_tokens_seen": 12000880,
      "step": 1470
    },
    {
      "epoch": 2.36,
      "grad_norm": 1.66486394405365,
      "learning_rate": 5.4076635668540075e-06,
      "loss": 2.5031,
      "num_input_tokens_seen": 12040592,
      "step": 1475
    },
    {
      "epoch": 2.368,
      "grad_norm": 2.976120710372925,
      "learning_rate": 5.2782597305116504e-06,
      "loss": 2.5521,
      "num_input_tokens_seen": 12081200,
      "step": 1480
    },
    {
      "epoch": 2.376,
      "grad_norm": 1.5687960386276245,
      "learning_rate": 5.150240033804116e-06,
      "loss": 2.4822,
      "num_input_tokens_seen": 12122432,
      "step": 1485
    },
    {
      "epoch": 2.384,
      "grad_norm": 2.942143440246582,
      "learning_rate": 5.023613461594512e-06,
      "loss": 2.5273,
      "num_input_tokens_seen": 12162112,
      "step": 1490
    },
    {
      "epoch": 2.392,
      "grad_norm": 2.123303174972534,
      "learning_rate": 4.898388900971634e-06,
      "loss": 2.5109,
      "num_input_tokens_seen": 12203800,
      "step": 1495
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.9927492141723633,
      "learning_rate": 4.7745751406263165e-06,
      "loss": 2.5118,
      "num_input_tokens_seen": 12245312,
      "step": 1500
    },
    {
      "epoch": 2.408,
      "grad_norm": 1.836093544960022,
      "learning_rate": 4.6521808702345514e-06,
      "loss": 2.5045,
      "num_input_tokens_seen": 12285824,
      "step": 1505
    },
    {
      "epoch": 2.416,
      "grad_norm": 1.989782691001892,
      "learning_rate": 4.53121467984764e-06,
      "loss": 2.4606,
      "num_input_tokens_seen": 12327232,
      "step": 1510
    },
    {
      "epoch": 2.424,
      "grad_norm": 1.4428343772888184,
      "learning_rate": 4.411685059289314e-06,
      "loss": 2.4678,
      "num_input_tokens_seen": 12368352,
      "step": 1515
    },
    {
      "epoch": 2.432,
      "grad_norm": 2.42268967628479,
      "learning_rate": 4.293600397559897e-06,
      "loss": 2.5046,
      "num_input_tokens_seen": 12408728,
      "step": 1520
    },
    {
      "epoch": 2.44,
      "grad_norm": 1.8376861810684204,
      "learning_rate": 4.176968982247514e-06,
      "loss": 2.5623,
      "num_input_tokens_seen": 12449016,
      "step": 1525
    },
    {
      "epoch": 2.448,
      "grad_norm": 2.6363725662231445,
      "learning_rate": 4.061798998946459e-06,
      "loss": 2.4949,
      "num_input_tokens_seen": 12490856,
      "step": 1530
    },
    {
      "epoch": 2.456,
      "grad_norm": 2.8489110469818115,
      "learning_rate": 3.948098530682695e-06,
      "loss": 2.5127,
      "num_input_tokens_seen": 12531712,
      "step": 1535
    },
    {
      "epoch": 2.464,
      "grad_norm": 1.892426609992981,
      "learning_rate": 3.835875557346552e-06,
      "loss": 2.5114,
      "num_input_tokens_seen": 12572416,
      "step": 1540
    },
    {
      "epoch": 2.472,
      "grad_norm": 1.709897518157959,
      "learning_rate": 3.725137955132707e-06,
      "loss": 2.5219,
      "num_input_tokens_seen": 12612136,
      "step": 1545
    },
    {
      "epoch": 2.48,
      "grad_norm": 1.502326250076294,
      "learning_rate": 3.6158934959873353e-06,
      "loss": 2.5245,
      "num_input_tokens_seen": 12652976,
      "step": 1550
    },
    {
      "epoch": 2.488,
      "grad_norm": 2.3314900398254395,
      "learning_rate": 3.508149847062725e-06,
      "loss": 2.4812,
      "num_input_tokens_seen": 12692760,
      "step": 1555
    },
    {
      "epoch": 2.496,
      "grad_norm": 1.4575321674346924,
      "learning_rate": 3.4019145701791184e-06,
      "loss": 2.4683,
      "num_input_tokens_seen": 12733488,
      "step": 1560
    },
    {
      "epoch": 2.504,
      "grad_norm": 2.226381540298462,
      "learning_rate": 3.297195121294022e-06,
      "loss": 2.5223,
      "num_input_tokens_seen": 12775216,
      "step": 1565
    },
    {
      "epoch": 2.512,
      "grad_norm": 2.8584444522857666,
      "learning_rate": 3.1939988499789077e-06,
      "loss": 2.5333,
      "num_input_tokens_seen": 12816128,
      "step": 1570
    },
    {
      "epoch": 2.52,
      "grad_norm": 1.0445666313171387,
      "learning_rate": 3.092332998903416e-06,
      "loss": 2.4969,
      "num_input_tokens_seen": 12857712,
      "step": 1575
    },
    {
      "epoch": 2.528,
      "grad_norm": 2.5231125354766846,
      "learning_rate": 2.992204703326995e-06,
      "loss": 2.4523,
      "num_input_tokens_seen": 12901184,
      "step": 1580
    },
    {
      "epoch": 2.536,
      "grad_norm": 1.150184988975525,
      "learning_rate": 2.893620990598192e-06,
      "loss": 2.4842,
      "num_input_tokens_seen": 12942288,
      "step": 1585
    },
    {
      "epoch": 2.544,
      "grad_norm": 1.9233132600784302,
      "learning_rate": 2.7965887796613884e-06,
      "loss": 2.5259,
      "num_input_tokens_seen": 12983800,
      "step": 1590
    },
    {
      "epoch": 2.552,
      "grad_norm": 2.8242149353027344,
      "learning_rate": 2.7011148805712314e-06,
      "loss": 2.5253,
      "num_input_tokens_seen": 13023480,
      "step": 1595
    },
    {
      "epoch": 2.56,
      "grad_norm": 2.082491636276245,
      "learning_rate": 2.6072059940146775e-06,
      "loss": 2.4816,
      "num_input_tokens_seen": 13065120,
      "step": 1600
    },
    {
      "epoch": 2.568,
      "grad_norm": 2.0616185665130615,
      "learning_rate": 2.514868710840723e-06,
      "loss": 2.465,
      "num_input_tokens_seen": 13106624,
      "step": 1605
    },
    {
      "epoch": 2.576,
      "grad_norm": 2.279850959777832,
      "learning_rate": 2.424109511597822e-06,
      "loss": 2.4923,
      "num_input_tokens_seen": 13147528,
      "step": 1610
    },
    {
      "epoch": 2.584,
      "grad_norm": 2.2945010662078857,
      "learning_rate": 2.3349347660790582e-06,
      "loss": 2.4583,
      "num_input_tokens_seen": 13188784,
      "step": 1615
    },
    {
      "epoch": 2.592,
      "grad_norm": 3.2804393768310547,
      "learning_rate": 2.2473507328751086e-06,
      "loss": 2.5055,
      "num_input_tokens_seen": 13227528,
      "step": 1620
    },
    {
      "epoch": 2.6,
      "grad_norm": 2.8839948177337646,
      "learning_rate": 2.1613635589349756e-06,
      "loss": 2.5101,
      "num_input_tokens_seen": 13268416,
      "step": 1625
    },
    {
      "epoch": 2.608,
      "grad_norm": 1.6164857149124146,
      "learning_rate": 2.0769792791345945e-06,
      "loss": 2.4769,
      "num_input_tokens_seen": 13308936,
      "step": 1630
    },
    {
      "epoch": 2.616,
      "grad_norm": 2.3325388431549072,
      "learning_rate": 1.9942038158532407e-06,
      "loss": 2.5173,
      "num_input_tokens_seen": 13348816,
      "step": 1635
    },
    {
      "epoch": 2.624,
      "grad_norm": 2.871101140975952,
      "learning_rate": 1.913042978557944e-06,
      "loss": 2.4975,
      "num_input_tokens_seen": 13389696,
      "step": 1640
    },
    {
      "epoch": 2.632,
      "grad_norm": 2.0607242584228516,
      "learning_rate": 1.8335024633956976e-06,
      "loss": 2.5176,
      "num_input_tokens_seen": 13430248,
      "step": 1645
    },
    {
      "epoch": 2.64,
      "grad_norm": 2.3220701217651367,
      "learning_rate": 1.7555878527937164e-06,
      "loss": 2.5311,
      "num_input_tokens_seen": 13470520,
      "step": 1650
    },
    {
      "epoch": 2.648,
      "grad_norm": 2.289459466934204,
      "learning_rate": 1.679304615067634e-06,
      "loss": 2.5248,
      "num_input_tokens_seen": 13509808,
      "step": 1655
    },
    {
      "epoch": 2.656,
      "grad_norm": 3.580810546875,
      "learning_rate": 1.6046581040377319e-06,
      "loss": 2.5885,
      "num_input_tokens_seen": 13549616,
      "step": 1660
    },
    {
      "epoch": 2.664,
      "grad_norm": 1.1437163352966309,
      "learning_rate": 1.5316535586531483e-06,
      "loss": 2.4503,
      "num_input_tokens_seen": 13592432,
      "step": 1665
    },
    {
      "epoch": 2.672,
      "grad_norm": 1.7371668815612793,
      "learning_rate": 1.4602961026242479e-06,
      "loss": 2.4714,
      "num_input_tokens_seen": 13633160,
      "step": 1670
    },
    {
      "epoch": 2.68,
      "grad_norm": 2.3360908031463623,
      "learning_rate": 1.3905907440629752e-06,
      "loss": 2.4515,
      "num_input_tokens_seen": 13672576,
      "step": 1675
    },
    {
      "epoch": 2.6879999999999997,
      "grad_norm": 1.9524301290512085,
      "learning_rate": 1.3225423751313942e-06,
      "loss": 2.5277,
      "num_input_tokens_seen": 13714200,
      "step": 1680
    },
    {
      "epoch": 2.6959999999999997,
      "grad_norm": 1.8480896949768066,
      "learning_rate": 1.2561557716983307e-06,
      "loss": 2.5291,
      "num_input_tokens_seen": 13753568,
      "step": 1685
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 1.877863883972168,
      "learning_rate": 1.1914355930041837e-06,
      "loss": 2.5078,
      "num_input_tokens_seen": 13792632,
      "step": 1690
    },
    {
      "epoch": 2.7119999999999997,
      "grad_norm": 2.406342029571533,
      "learning_rate": 1.1283863813339263e-06,
      "loss": 2.5492,
      "num_input_tokens_seen": 13835368,
      "step": 1695
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 2.501929521560669,
      "learning_rate": 1.067012561698319e-06,
      "loss": 2.4843,
      "num_input_tokens_seen": 13876240,
      "step": 1700
    },
    {
      "epoch": 2.7279999999999998,
      "grad_norm": 2.356426477432251,
      "learning_rate": 1.0073184415233333e-06,
      "loss": 2.4499,
      "num_input_tokens_seen": 13916432,
      "step": 1705
    },
    {
      "epoch": 2.7359999999999998,
      "grad_norm": 1.4272712469100952,
      "learning_rate": 9.493082103478517e-07,
      "loss": 2.4635,
      "num_input_tokens_seen": 13956640,
      "step": 1710
    },
    {
      "epoch": 2.7439999999999998,
      "grad_norm": 2.6260576248168945,
      "learning_rate": 8.929859395296364e-07,
      "loss": 2.5241,
      "num_input_tokens_seen": 14000872,
      "step": 1715
    },
    {
      "epoch": 2.752,
      "grad_norm": 1.878997564315796,
      "learning_rate": 8.383555819595601e-07,
      "loss": 2.4876,
      "num_input_tokens_seen": 14041096,
      "step": 1720
    },
    {
      "epoch": 2.76,
      "grad_norm": 1.7078970670700073,
      "learning_rate": 7.854209717842231e-07,
      "loss": 2.4748,
      "num_input_tokens_seen": 14080400,
      "step": 1725
    },
    {
      "epoch": 2.768,
      "grad_norm": 1.693261742591858,
      "learning_rate": 7.341858241368183e-07,
      "loss": 2.5356,
      "num_input_tokens_seen": 14124160,
      "step": 1730
    },
    {
      "epoch": 2.776,
      "grad_norm": 3.314697504043579,
      "learning_rate": 6.846537348764115e-07,
      "loss": 2.5555,
      "num_input_tokens_seen": 14164216,
      "step": 1735
    },
    {
      "epoch": 2.784,
      "grad_norm": 2.212780475616455,
      "learning_rate": 6.368281803355691e-07,
      "loss": 2.5249,
      "num_input_tokens_seen": 14204984,
      "step": 1740
    },
    {
      "epoch": 2.792,
      "grad_norm": 2.7207438945770264,
      "learning_rate": 5.907125170763806e-07,
      "loss": 2.5261,
      "num_input_tokens_seen": 14250128,
      "step": 1745
    },
    {
      "epoch": 2.8,
      "grad_norm": 2.1197128295898438,
      "learning_rate": 5.463099816548579e-07,
      "loss": 2.4958,
      "num_input_tokens_seen": 14290056,
      "step": 1750
    },
    {
      "epoch": 2.808,
      "grad_norm": 1.4524132013320923,
      "learning_rate": 5.036236903938285e-07,
      "loss": 2.5065,
      "num_input_tokens_seen": 14331320,
      "step": 1755
    },
    {
      "epoch": 2.816,
      "grad_norm": 2.7912373542785645,
      "learning_rate": 4.6265663916417735e-07,
      "loss": 2.5242,
      "num_input_tokens_seen": 14372904,
      "step": 1760
    },
    {
      "epoch": 2.824,
      "grad_norm": 2.0597054958343506,
      "learning_rate": 4.234117031746143e-07,
      "loss": 2.5241,
      "num_input_tokens_seen": 14413912,
      "step": 1765
    },
    {
      "epoch": 2.832,
      "grad_norm": 1.7430702447891235,
      "learning_rate": 3.8589163676986674e-07,
      "loss": 2.4877,
      "num_input_tokens_seen": 14455176,
      "step": 1770
    },
    {
      "epoch": 2.84,
      "grad_norm": 1.8435637950897217,
      "learning_rate": 3.5009907323737825e-07,
      "loss": 2.4833,
      "num_input_tokens_seen": 14496912,
      "step": 1775
    },
    {
      "epoch": 2.848,
      "grad_norm": 1.6966794729232788,
      "learning_rate": 3.1603652462249e-07,
      "loss": 2.5245,
      "num_input_tokens_seen": 14537640,
      "step": 1780
    },
    {
      "epoch": 2.856,
      "grad_norm": 1.667269229888916,
      "learning_rate": 2.8370638155215123e-07,
      "loss": 2.461,
      "num_input_tokens_seen": 14578056,
      "step": 1785
    },
    {
      "epoch": 2.864,
      "grad_norm": 1.0208218097686768,
      "learning_rate": 2.531109130671061e-07,
      "loss": 2.4776,
      "num_input_tokens_seen": 14618584,
      "step": 1790
    },
    {
      "epoch": 2.872,
      "grad_norm": 2.21191143989563,
      "learning_rate": 2.2425226646268227e-07,
      "loss": 2.5145,
      "num_input_tokens_seen": 14659056,
      "step": 1795
    },
    {
      "epoch": 2.88,
      "grad_norm": 2.10197377204895,
      "learning_rate": 1.9713246713805588e-07,
      "loss": 2.4933,
      "num_input_tokens_seen": 14698968,
      "step": 1800
    },
    {
      "epoch": 2.888,
      "grad_norm": 2.7987749576568604,
      "learning_rate": 1.717534184541153e-07,
      "loss": 2.4779,
      "num_input_tokens_seen": 14739760,
      "step": 1805
    },
    {
      "epoch": 2.896,
      "grad_norm": 2.694730281829834,
      "learning_rate": 1.4811690159988455e-07,
      "loss": 2.5254,
      "num_input_tokens_seen": 14781456,
      "step": 1810
    },
    {
      "epoch": 2.904,
      "grad_norm": 2.659358024597168,
      "learning_rate": 1.2622457546749567e-07,
      "loss": 2.5658,
      "num_input_tokens_seen": 14822760,
      "step": 1815
    },
    {
      "epoch": 2.912,
      "grad_norm": 2.054035186767578,
      "learning_rate": 1.0607797653577334e-07,
      "loss": 2.5452,
      "num_input_tokens_seen": 14866808,
      "step": 1820
    },
    {
      "epoch": 2.92,
      "grad_norm": 1.5609747171401978,
      "learning_rate": 8.767851876239074e-08,
      "loss": 2.4915,
      "num_input_tokens_seen": 14907632,
      "step": 1825
    },
    {
      "epoch": 2.928,
      "grad_norm": 2.4397027492523193,
      "learning_rate": 7.102749348465165e-08,
      "loss": 2.5172,
      "num_input_tokens_seen": 14948104,
      "step": 1830
    },
    {
      "epoch": 2.936,
      "grad_norm": 2.0362155437469482,
      "learning_rate": 5.612606932883513e-08,
      "loss": 2.521,
      "num_input_tokens_seen": 14986624,
      "step": 1835
    },
    {
      "epoch": 2.944,
      "grad_norm": 2.206249713897705,
      "learning_rate": 4.2975292128200064e-08,
      "loss": 2.5394,
      "num_input_tokens_seen": 15027320,
      "step": 1840
    },
    {
      "epoch": 2.952,
      "grad_norm": 2.8000595569610596,
      "learning_rate": 3.157608484956332e-08,
      "loss": 2.5033,
      "num_input_tokens_seen": 15068848,
      "step": 1845
    },
    {
      "epoch": 2.96,
      "grad_norm": 1.8621776103973389,
      "learning_rate": 2.192924752854042e-08,
      "loss": 2.5044,
      "num_input_tokens_seen": 15109824,
      "step": 1850
    },
    {
      "epoch": 2.968,
      "grad_norm": 2.1195578575134277,
      "learning_rate": 1.4035457213393276e-08,
      "loss": 2.5416,
      "num_input_tokens_seen": 15150600,
      "step": 1855
    },
    {
      "epoch": 2.976,
      "grad_norm": 1.657591700553894,
      "learning_rate": 7.895267917501504e-09,
      "loss": 2.5675,
      "num_input_tokens_seen": 15192024,
      "step": 1860
    },
    {
      "epoch": 2.984,
      "grad_norm": 1.4843804836273193,
      "learning_rate": 3.5091105804907487e-09,
      "loss": 2.4948,
      "num_input_tokens_seen": 15232152,
      "step": 1865
    },
    {
      "epoch": 2.992,
      "grad_norm": 1.752678394317627,
      "learning_rate": 8.772930379846722e-10,
      "loss": 2.4764,
      "num_input_tokens_seen": 15271840,
      "step": 1870
    },
    {
      "epoch": 3.0,
      "grad_norm": 2.23873233795166,
      "learning_rate": 0.0,
      "loss": 2.5158,
      "num_input_tokens_seen": 15313272,
      "step": 1875
    },
    {
      "epoch": 3.0,
      "num_input_tokens_seen": 15313272,
      "step": 1875,
      "total_flos": 2.443169103830057e+17,
      "train_loss": 2.5962091588338216,
      "train_runtime": 2742.9651,
      "train_samples_per_second": 10.937,
      "train_steps_per_second": 0.684
    }
  ],
  "logging_steps": 5,
  "max_steps": 1875,
  "num_input_tokens_seen": 15313272,
  "num_train_epochs": 3,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.443169103830057e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
